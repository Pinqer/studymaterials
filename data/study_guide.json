{
  "studyGuide": [
    {
      "id": "study-guide-full",
      "title": "Comprehensive Study Guide",
      "category": "All Topics",
      "content": "# 1MS041 Exam Preparation - Generated Questions and Answers\n\nThis document contains newly created variants of exercises based on previous exams (2022-2024) and assignments. The focus is on Markov Chains, Maximum Likelihood Estimation (MLE), Sampling (Rejection/Inversion), Classification, and Concentration of Measure.\n\n---\n\n## Category: Markov Chains\n\n### Problem 1: Customer Behavior on Website\n**Description:**\nAn e-commerce website models user behavior with three states: **Browsing (B)**, **Cart (C)**, and **Purchased (P)**. The transition probabilities are as follows:\n* From **Browsing**: 60% stay, 30% go to Cart, 10% leave (we ignore leaving for this closed chain and normalize: 0.6, 0.3, 0.1 distributed over B, C, P for simplicity; let's assume a closed model where P returns to B).\n* Let's define the matrix $P$ exactly:\n    * $P_{B \\to B} = 0.5, P_{B \\to C} = 0.4, P_{B \\to P} = 0.1$\n    * $P_{C \\to B} = 0.3, P_{C \\to C} = 0.2, P_{C \\to P} = 0.5$\n    * $P_{P \\to B} = 0.8, P_{P \\to C} = 0.0, P_{P \\to P} = 0.2$\n\n**Tasks:**\n1. Define the transition matrix in Python.\n2. Compute the stationary distribution.\n3. If a user starts in \"Browsing\", what is the probability they are in \"Purchased\" after exactly 3 steps?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Definiera övergångsmatrisen\n# Ordning: [Browsing, Cart, Purchased]\nP = np.array([\n    [0.5, 0.4, 0.1],\n    [0.3, 0.2, 0.5],\n    [0.8, 0.0, 0.2]\n])\n\nprint(\"Transition Matrix P:\\n\", P)\n\n# 2. Calculate stationary distribution\n# Solve pi * P = pi, which is the same as (P.T - I) * pi = 0\n# We add the condition that the sum of pi is 1.\neig_vals, eig_vecs = np.linalg.eig(P.T)\n# Find the eigenvector corresponding to eigenvalue 1 (or closest to 1)\nstationary_idx = np.argmin(np.abs(eig_vals - 1.0))\nstationary_vec = np.real(eig_vecs[:, stationary_idx])\nstationary_dist = stationary_vec / np.sum(stationary_vec)\n\nprint(\"\\nStationary Distribution (pi):\", stationary_dist)\n# Expected result (approximately): [0.48, 0.23, 0.29]\n\n# 3. Probability of being in Purchased after 3 steps starting from Browsing\n# Start vector (1, 0, 0)\nstart_state = np.array([1, 0, 0])\n# P after 3 steps is P^3\nP_3 = np.linalg.matrix_power(P, 3)\nprob_after_3 = np.dot(start_state, P_3)\n\nprint(\"\\nProbability distribution after 3 steps:\", prob_after_3)\nprint(\"Probability of being in 'Purchased' after 3 steps:\", prob_after_3[2])\n\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n### Problem 2: Estimation of Rayleigh Distribution\n\n**Description:**\nYou have collected data that is assumed to follow a Rayleigh distribution with probability density function:\n$$ f(x; \\sigma) = \\frac{x}{\\sigma^2} e^{-x^2 / (2\\sigma^2)}, \\quad x \\geq 0 $$\nYou have 50 observations. Write a function to numerically find the MLE for the parameter $\\sigma$.\n\n**Tasks:**\n\n1. Generate synthetic data (True $\\sigma$).\n2. Define the negative log-likelihood function.\n3. Minimize the function to find $\\sigma$.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\n\n# 1. Generate data\nnp.random.seed(42)\ntrue_sigma = 2.5\n# Rayleigh in numpy uses the 'scale' parameter as sigma\ndata = np.random.rayleigh(scale=true_sigma, size=50)\n\n# 2. Define negative log-likelihood\ndef neg_log_likelihood(params, x):\n    sigma = params[0]\n    if sigma <= 0: return np.inf # Constraint\n    \n    n = len(x)\n    # Log-likelihood L = sum(ln(x) - 2ln(sigma) - x^2/(2sigma^2))\n    # We can ignore sum(ln(x)) when minimizing since it doesn't depend on sigma, but we include it for completeness.\n    log_l = np.sum(np.log(x) - 2*np.log(sigma) - (x**2)/(2*sigma**2))\n    return -log_l\n\n# 3. Optimize\ninitial_guess = [1.0]\nresult = optimize.minimize(\n    neg_log_likelihood, \n    initial_guess, \n    args=(data,), \n    bounds=[(0.01, None)], # Sigma must be positive\n    method='L-BFGS-B'\n)\n\nestimated_sigma = result.x[0]\nprint(f\"True sigma: {true_sigma}\")\nprint(f\"Estimated sigma: {estimated_sigma:.4f}\")\n\n# Analytical solution for Rayleigh is sigma_hat = sqrt( sum(x^2) / (2N) )\nanalytical_sigma = np.sqrt(np.sum(data**2) / (2 * len(data)))\nprint(f\"Analytical check: {analytical_sigma:.4f}\")\n\n```\n\n---\n\n## Category: Sampling & Monte Carlo\n\n### Problem 3: Accept-Reject Sampling\n\n**Description:**\nWe want to sample from the distribution $f(x) = 3x^2$ for $x \\in [0,1]$.\nUse a Uniform(0,1) distribution as the proposal distribution $g(x)$.\n\n**Tasks:**\n\n1. Determine the constant $M$ so that $f(x) \\leq M g(x)$ for all $x$.\n2. Implement an `accept_reject` function that generates 10,000 samples.\n3. Calculate the integral $E[X]$ (i.e., the expected value $E[X]$) using Monte Carlo integration based on your samples.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Determine M\n# f(x) = 3x^2 on [0,1]. Maximum is at x=1, where f(1)=3.\n# g(x) = 1.\n# We must have 3x^2 <= M * 1. M = 3 is the minimum possible value.\nM = 3\n\ndef target_f(x):\n    return 3 * x**2\n\ndef accept_reject(n_samples):\n    samples = []\n    while len(samples) < n_samples:\n        # Generate proposal from Uniform(0,1)\n        x_prop = np.random.uniform(0, 1)\n        # Generate u from Uniform(0,1)\n        u = np.random.uniform(0, 1)\n        \n        # Acceptance criterion: u <= f(x) / (M * g(x))\n        if u <= target_f(x_prop) / (M * 1):\n            samples.append(x_prop)\n            \n    return np.array(samples)\n\n# 2. Generate samples\ngenerated_samples = accept_reject(10000)\n\n# Visualize (optional but good for verification)\n# plt.hist(generated_samples, bins=50, density=True, alpha=0.6, label='Samples')\n# xx = np.linspace(0,1,100)\n# plt.plot(xx, target_f(xx), 'r', label='True PDF')\n# plt.show()\n\n# 3. Monte Carlo Integration for E[X]\n# Integral x * f(x) dx is approximated by mean(samples) since samples are drawn from f(x).\nmonte_carlo_mean = np.mean(generated_samples)\ntrue_mean = 0.75 # Integral of x * 3x^2 = 3x^3 -> [3x^4/4]0..1 = 3/4\n\nprint(f\"Monte Carlo Estimated E[X]: {monte_carlo_mean:.4f}\")\nprint(f\"True E[X]: {true_mean}\")\n\n```\n\n---\n\n## Category: Classification and Confidence Intervals\n\n### Problem 4: Cost-Sensitive Classification and Hoeffding\n\n**Description:**\nYou have a model that predicts fraud (Fraud=1, Normal=0).\nCost matrix:\n\n* False Positive (FP): Cost 10 (Annoy customer)\n* False Negative (FN): Cost 100 (Lost money)\n* TP and TN: Cost 0\n\nYou have 1000 test points. Your model gives probabilities `y_proba`.\n\n**Tasks:**\n\n1. Write a function `calculate_cost(y_true, y_pred)` that calculates the total cost.\n2. Find the threshold (threshold) $t$ that minimizes cost on the test set.\n3. Calculate a 95% confidence interval for accuracy (accuracy) at this optimal threshold using Hoeffding's inequality.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Simulera data\nnp.random.seed(99)\nn_test = 1000\ny_true = np.random.binomial(1, 0.05, n_test) # 5% fraud\n# Simulera modell-sannolikheter (lite brusig men korrelerad)\ny_proba = np.random.uniform(0, 1, n_test)\ny_proba[y_true == 1] = np.random.beta(5, 1, np.sum(y_true == 1)) # Fraud har högre prob\ny_proba[y_true == 0] = np.random.beta(1, 5, np.sum(y_true == 0)) # Normal har lägre prob\n\n# 1. Kostnadsfunktion\ndef calculate_cost(y_true, y_pred):\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    cost = fp * 10 + fn * 100\n    return cost\n\n# 2. Hitta optimal threshold\nthresholds = np.linspace(0, 1, 101)\nbest_cost = float('inf')\nbest_t = 0.5\n\nfor t in thresholds:\n    y_pred_t = (y_proba >= t).astype(int)\n    current_cost = calculate_cost(y_true, y_pred_t)\n    if current_cost < best_cost:\n        best_cost = current_cost\n        best_t = t\n\nprint(f\"Optimal Threshold: {best_t}\")\nprint(f\"Minimal Cost: {best_cost}\")\n\n# 3. Hoeffding Intervall för Accuracy\n# Välj optimala prediktioner\nbest_preds = (y_proba >= best_t).astype(int)\nacc = accuracy_score(y_true, best_preds)\nn = len(y_true)\nalpha = 0.05 # 95% konfidens -> 5% felrisk\n\n# Hoeffding epsilon: sqrt(ln(2/alpha) / (2n))\nepsilon = np.sqrt(np.log(2 / alpha) / (2 * n))\nci_lower = max(0, acc - epsilon)\nci_upper = min(1, acc + epsilon)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"95% CI (Hoeffding): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n\n```\n\n---\n\n## Category: Text Analysis and Data Handling\n\n### Problem 5: Bag-of-Words and Probability\n\n**Description:**\nGiven a list of SMS messages, calculate the conditional probability $P(Spam | \\text{'win' in text})$.\nUse `CountVectorizer` to identify the word.\n\n**Tasks:**\n\n1. Prepare the data.\n2. Create a binary vector for whether the word \"win\" exists in each text.\n3. Calculate the empirical probability.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Exempeldata\ntexts = [\n    \"Win a free prize now\",\n    \"Meeting at noon\",\n    \"You have won a lottery win\",\n    \"Can we talk later?\",\n    \"Win big money\",\n    \"Project deadline tomorrow\"\n]\n# 1 = Spam, 0 = Ham\nlabels = np.array([1, 0, 1, 0, 1, 0])\n\n# 1. Vectorizer (binary=True för existens snarare än antal)\nvectorizer = CountVectorizer(binary=True)\nX = vectorizer.fit_transform(texts)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Hitta index för ordet \"win\"\ntry:\n    win_index = np.where(feature_names == \"win\")[0][0]\nexcept IndexError:\n    print(\"Ordet 'win' finns inte i vokabulären.\")\n    win_index = None\n\nif win_index is not None:\n    # 2. Hitta vilka texter som innehåller \"win\"\n    # X är en sparse matrix, hämta kolumnen för \"win\"\n    has_win = X[:, win_index].toarray().flatten()\n    \n    # 3. Beräkna P(Spam | \"win\")\n    # P(A|B) = P(A och B) / P(B) -> Antal(Spam och Win) / Antal(Win)\n    num_win = np.sum(has_win)\n    num_spam_and_win = np.sum(labels[has_win == 1])\n    \n    if num_win > 0:\n        p_spam_given_win = num_spam_and_win / num_win\n        print(f\"Antal texter med 'win': {num_win}\")\n        print(f\"Antal av dessa som är spam: {num_spam_and_win}\")\n        print(f\"P(Spam | 'win' in text) = {p_spam_given_win:.4f}\")\n    else:\n        print(\"Inga texter innehöll ordet 'win'.\")\n\n```\n\n---\n\n## Category: Concentration of Measure\n\n### Problem 6: Comparison of Concentrations\n\n**Description:**\nWhich of the following concentrates **exponentially** quickly toward its expected value as the number of samples $n$ increases?\n\n1. The empirical mean of i.i.d. variables with finite variance (but not bounded)?\n2. The empirical mean of i.i.d. bounded random variables (Bounded RVs)?\n3. The empirical mean of a Cauchy distribution?\n\n**Answer:**\n\n* **Option 2** concentrates exponentially. This is the core of Hoeffding's inequality ($P(|\\bar{X}_n - \\mu| > \\epsilon) \\leq 2e^{-2n\\epsilon^2/(b-a)^2}$).\n* Option 1 usually concentrates polynomially (via Chebyshev's inequality) if we only assume finite variance without stronger assumptions (like sub-Gaussian).\n* Option 3 (Cauchy) has no expected value and does not concentrate at all (Law of large numbers does not apply).\n\n**Code Example for Verification (Simulation):**\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN_experiments = 1000\nsample_sizes = [10, 100, 500, 1000]\nthreshold = 0.1\n\nprint(\"Probability that deviation > 0.1 for different n (Bounded vs Pareto):\")\n\nfor n in sample_sizes:\n    # Bounded (Uniform 0,1), Mean = 0.5\n    bounded_means = np.mean(np.random.uniform(0, 1, (N_experiments, n)), axis=1)\n    prob_bounded = np.mean(np.abs(bounded_means - 0.5) > threshold)\n    \n    # Heavy tail (Pareto, a=1.5), Mean exists but variance infinite/large\n    # Pareto mean = a / (a-1) = 3.0\n    # We use standard t-distribution with df=3 for \"finite variance but not bounded\"\n    # Mean = 0\n    heavy_means = np.mean(np.random.standard_t(df=3, size=(N_experiments, n)), axis=1)\n    prob_heavy = np.mean(np.abs(heavy_means - 0) > threshold)\n    \n    print(f\"n={n}: Bounded Prob={prob_bounded:.4f}, T-dist Prob={prob_heavy:.4f}\")\n\n# You can clearly see that Bounded Prob goes to 0 much faster than T-dist Prob.\n\n```\n\n# 1MS041 Exam Preparation - Round 2\n\nHere are additional newly created variants of exercises based on course material, with focus on Probability Theory, Vectors/Data Analysis, MLE, Text Analysis, and Optimization.\n\n---\n\n## Category: Probability and Conditioning\n\n### Problem 7: Quality Control in Factory (Binomial Distribution)\n**Description:**\nA factory produces batches of 50 components. The number of defective components in a batch, $N$, is assumed to follow a binomial distribution $N \\sim \\text{Bin}(50, 0.05)$.\nThe factory has an automatic testing system that raises an alarm (discards the batch) if the number of detected defects $Y \\ge T$.\nHowever, the system is not perfect. If a component is defective, it is detected with 90% probability. If a component is intact, it is falsely marked as defective with 1% probability.\nLet $Y$ be the number of *reported* defects.\n\n**Tasks:**\n1. Simulate the process for 100,000 batches to estimate the distribution of $Y$.\n2. Set the threshold $T=5$. Calculate the conditional probability that a batch actually has fewer than 2 defective components ($N < 2$) given that the system raised an alarm ($Y \\ge 5$).\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Parameters\nn_items = 50\np_defect = 0.05\np_detect_given_defect = 0.90\np_alarm_given_healthy = 0.01\nn_sim = 100000\nT = 5\n\n# 1. Simulation\n# N: Number of actually defective components in each batch\nN = np.random.binomial(n_items, p_defect, n_sim)\n\n# Y: Number of reported defects\n# Y consists of detected defectives (True Positives) + false positives (False Positives)\n# Number of intact = n_items - N\nTP = np.random.binomial(N, p_detect_given_defect)\nFP = np.random.binomial(n_items - N, p_alarm_given_healthy)\nY = TP + FP\n\n# 2. Conditional probability P(N < 2 | Y >= T)\n# Filter out cases where the alarm went off\nalarm_indices = Y >= T\nN_given_alarm = N[alarm_indices]\n\n# Calculate the fraction where N < 2\nprob_N_less_2_given_alarm = np.mean(N_given_alarm < 2)\n\nprint(f\"Estimated P(N < 2 | Y >= {T}) = {prob_N_less_2_given_alarm:.4f}\")\n\n```\n\n---\n\n## Category: Data Analysis and Linear Algebra\n\n### Problem 8: Motion Analysis and Covariance\n\n**Description:**\nYou have data about a robot's position at 100 time points. The data is in variables `x_pos` and `y_pos`.\nYou should analyze the robot's motion.\n\n**Tasks:**\n\n1. Create a numpy array `positions` of size (2, 100).\n2. Calculate the **mean position** (centroid).\n3. Calculate the **empirical covariance matrix** for the positions (should be 2x2).\n4. Calculate the distance from each point to the mean position and state the average distance.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Generate synthetic data (correlated motion)\nnp.random.seed(42)\nx_pos = np.random.normal(10, 2, 100)\ny_pos = 0.5 * x_pos + np.random.normal(0, 1, 100)\n\n# 1. Create array\npositions = np.vstack((x_pos, y_pos))\nprint(f\"Shape: {positions.shape}\") # Should be (2, 100)\n\n# 2. Mean position\nmean_pos = np.mean(positions, axis=1)\nprint(f\"Mean position (x, y): {mean_pos}\")\n\n# 3. Covariance matrix\n# bias=True for empirical covariance (divided by N), bias=False for N-1 (more common in statistics)\n# The task says \"empirical\", often 1/N, but np.cov defaults to 1/(N-1). We use standard np.cov.\ncov_matrix = np.cov(positions)\nprint(\"Covariance matrix:\\n\", cov_matrix)\n\n# 4. Average distance to mean position\n# Center the data\ncentered_pos = positions.T - mean_pos\n# Euclidean distance for each point (norm of vectors)\ndistances = np.linalg.norm(centered_pos, axis=1)\navg_distance = np.mean(distances)\n\nprint(f\"Average distance to centroid: {avg_distance:.4f}\")\n\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n### Problem 9: MLE for Exponential Distribution\n\n**Description:**\nThe time between arrivals to a server is assumed to follow an exponential distribution with probability density function:\n$$ f(x; \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0 $$\nYou have a list `arrival_times` with $n$ observations.\n\n**Tasks:**\n\n1. Derive (on paper/theoretically) the MLE for $\\lambda$. (Answer: $\\lambda_{MLE} = 1/\\bar{x}$).\n2. Write a function `mle_exponential(data)` that returns the estimate given the data.\n3. Use `scipy.optimize.minimize` to numerically find $\\lambda$ by minimizing negative log-likelihood and verify that it matches the analytical solution.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\n\n# Synthetic data (True lambda = 0.5)\ntrue_lambda = 0.5\ndata = np.random.exponential(1/true_lambda, 1000)\n\n# 1 & 2. Analytical solution\ndef mle_exponential(data):\n    # lambda_hat = 1 / mean(x)\n    return 1.0 / np.mean(data)\n\nanalytical_est = mle_exponential(data)\nprint(f\"Analytical estimate: {analytical_est:.4f}\")\n\n# 3. Numerical solution\ndef neg_log_likelihood(params, x):\n    lam = params[0]\n    if lam <= 0: return np.inf\n    # L = n*ln(lambda) - lambda * sum(x)\n    n = len(x)\n    log_l = n * np.log(lam) - lam * np.sum(x)\n    return -log_l\n\nres = optimize.minimize(\n    neg_log_likelihood, \n    x0=[1.0], \n    args=(data,), \n    bounds=[(0.001, None)]\n)\nnumerical_est = res.x[0]\n\nprint(f\"Numerical estimate:  {numerical_est:.4f}\")\nprint(f\"Difference: {abs(analytical_est - numerical_est):.2e}\")\n\n```\n\n---\n\n## Category: Text Analysis and Confidence Intervals\n\n### Problem 10: Probability Estimation in SMS Data\n\n**Description:**\nYou have a large collection of SMS data classified as Spam (1) or Not Spam (0).\nYou want to estimate the probability $P(Spam | \\text{'call' in text})$.\n\n**Tasks:**\n\n1. Use `CountVectorizer` to create a matrix of word occurrences.\n2. Calculate the point estimate $\\hat{p}$ for the above probability.\n3. Calculate a 95% confidence interval for this probability using **Hoeffding's inequality**.\n*Remember:* Hoeffding's interval for a mean $\\hat{p}$ is $[\\hat{p} - \\epsilon, \\hat{p} + \\epsilon]$ where $\\epsilon = \\sqrt{\\ln(2/\\alpha)/(2n)}$. Here $n$ is the number of SMS containing the word \"call\".\n\n**Solution and Code:**\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Example data\nsms_corpus = [\n    \"Call me later\", \n    \"You won a prize call now\", \n    \"Call for free money\", \n    \"Meeting at 10\", \n    \"Please call back\",\n    \"URGENT call now\"\n]\n# 1 = Spam, 0 = Ham\ny = np.array([0, 1, 1, 0, 0, 1])\n\n# 1. Vectorizer\nvec = CountVectorizer(binary=True) # binary=True because we only care if the word exists\nX = vec.fit_transform(sms_corpus)\nfeat_names = vec.get_feature_names_out()\n\ntarget_word = \"call\"\nif target_word in feat_names:\n    idx = np.where(feat_names == target_word)[0][0]\n    \n    # Find which documents contain the word\n    has_word = X[:, idx].toarray().flatten() == 1\n    \n    # Filter y based on these documents\n    y_subset = y[has_word]\n    n = len(y_subset)\n    \n    if n > 0:\n        # 2. Point estimate\n        p_hat = np.mean(y_subset)\n        \n        # 3. Hoeffding's interval\n        alpha = 0.05\n        epsilon = np.sqrt(np.log(2/alpha) / (2 * n))\n        \n        ci_lower = max(0, p_hat - epsilon)\n        ci_upper = min(1, p_hat + epsilon)\n        \n        print(f\"The word '{target_word}' was found in {n} messages.\")\n        print(f\"Point estimate p_hat: {p_hat:.4f}\")\n        print(f\"95% CI (Hoeffding): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n    else:\n        print(f\"The word '{target_word}' was not found in any messages.\")\nelse:\n    print(f\"The word '{target_word}' is not in the vocabulary.\")\n\n```\n\n---\n\n## Category: Optimization and Machine Learning\n\n### Problem 11: Implement Loss Function for Logistic Regression\n\n**Description:**\nIn the course, a \"Proportional Model\" (Logistic Regression) is often used where $\\hat{y}_i = \\sigma(\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip})$.\nTo train this model, we need to minimize the negative log-likelihood (Loss function).\n\n**Tasks:**\n\n1. Create a class `LogisticModel`.\n2. Implement the method `loss(coeffs, X, Y)`.\n* `coeffs`: Array where `coeffs[0]` is the intercept ($\\beta_0$) and the rest are weights ($\\beta_1, ...$).\n* The loss function for $N$ observations is:\n$$ J(\\beta) = - \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right] $$\nwhere $\\hat{y}_i = \\sigma(\\beta_0 + \\beta_1 x_{i1} + ...)$.\n\n3. Add a small regularization (Ridge/L2) to the loss function: $\\lambda \\sum_j \\beta_j^2$ (exclude the intercept if you want to be precise, but here we can include all for simplicity). $\\lambda = 0.1$.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nclass LogisticModel:\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def loss(self, coeffs, X, Y):\n        # coeffs[0] is intercept, coeffs[1:] are feature weights\n        intercept = coeffs[0]\n        beta = coeffs[1:]\n        \n        # Calculate linear combination z = beta*x + beta0\n        z = np.dot(X, beta) + intercept\n        \n        # Prediction (probability)\n        y_pred = self.sigmoid(z)\n        \n        # Avoid log(0) by clipping values\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        \n        # Negative Log Likelihood\n        # Formula: -sum(y*log(p) + (1-y)*log(1-p))\n        nll = -np.sum(Y * np.log(y_pred) + (1 - Y) * np.log(1 - y_pred))\n        \n        # L2 Regularization (lambda * sum(weights^2))\n        l2_lambda = 0.1\n        reg_term = l2_lambda * np.sum(coeffs**2)\n        \n        return nll + reg_term\n\n# Test of the function\nX_dummy = np.array([[1, 2], [2, 1], [0, 0]])\nY_dummy = np.array([1, 0, 0])\n# Guessed coefficients [intercept, w1, w2]\ncoeffs_guess = np.array([0.1, 0.5, -0.5])\n\nmodel = LogisticModel()\nloss_val = model.loss(coeffs_guess, X_dummy, Y_dummy)\nprint(f\"Calculated loss: {loss_val:.4f}\")\n\n```\n\n---\n\n# 1MS041 Exam Preparation - Additional Practice Problems (English)\n\n[cite_start]This section provides new practice problems in English, following the patterns seen in previous exams [cite: 14, 17, 18] [cite_start]and assignments[cite: 11, 13, 19].\n\n---\n\n## Category: Markov Chains\n\n### Problem 12: Network Server Reliability\n**Description:**\nA server can be in one of three states: **Operational (O)**, **Degraded (D)**, or **Failed (F)**. The transition probabilities per hour are:\n- From **Operational**: 80% stay Operational, 15% become Degraded, 5% Fail.\n- From **Degraded**: 0% become Operational (needs repair), 70% stay Degraded, 30% Fail.\n- From **Failed**: 100% become Operational after repair (takes exactly one hour).\n\n**Tasks:**\n1. [cite_start]Define the transition matrix $P$[cite: 10, 17].\n2. [cite_start]Determine if the chain is irreducible and aperiodic[cite: 18].\n3. [cite_start]Calculate the stationary distribution[cite: 14, 17].\n4. [cite_start]What is the expected number of hours until the server first fails, starting from Operational? [cite: 17, 10]\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Transition Matrix (States: O, D, F)\nP = np.array([\n    [0.80, 0.15, 0.05],\n    [0.00, 0.70, 0.30],\n    [1.00, 0.00, 0.00]\n])\n\n# 2. Properties\n# Irreducible: Yes, possible to reach any state from any state (O->D->F->O).\n# Aperiodic: Yes, P[0,0] > 0 (self-loop).\n\n# 3. Stationary Distribution\n# Solve pi * P = pi\nevals, evecs = np.linalg.eig(P.T)\npi = np.real(evecs[:, np.isclose(evals, 1)])\npi = pi / pi.sum()\nprint(f\"Stationary Distribution: {pi.flatten()}\")\n\n# 4. Expected Hitting Time to F starting from O\n# Solve system: E[T_O] = 1 + 0.8*E[T_O] + 0.15*E[T_D]\n#               E[T_D] = 1 + 0.7*E[T_D]\n# (Note: E[T_F] = 0)\n# From 2nd eq: 0.3*E[T_D] = 1 => E[T_D] = 10/3\n# Substitute into 1st: 0.2*E[T_O] = 1 + 0.15*(10/3) = 1 + 0.5 = 1.5\n# E[T_O] = 1.5 / 0.2 = 7.5 hours.\n\n# Matrix approach for confirmation\nQ = P[:2, :2] # Submatrix excluding state F\nI = np.eye(2)\nN = np.linalg.inv(I - Q) # Fundamental matrix\nhitting_times = N.dot(np.ones(2))\nprint(f\"Expected steps to hit F: from O={hitting_times[0]:.2f}, from D={hitting_times[1]:.2f}\")\n\n```\n\n---\n\n## Category: Maximum Likelihood Estimation\n\n### Problem 13: MLE for Zero-Truncated Poisson\n\n**Description:**\nIn some scenarios, we only observe counts greater than zero (e.g., number of items bought by a customer who actually entered the store). This follows a Zero-Truncated Poisson distribution:\n$$ P(X=k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k! (1 - e^{-\\lambda})}, \\quad k = 1, 2, \\dots $$\n\n**Tasks:**\n\n1. Implement the negative log-likelihood function.\n\n2. Numerically find the MLE $\\lambda$ for the dataset `data = [1, 2, 1, 3, 2, 4, 1, 2]`.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import factorial\n\ndata = np.array([1, 2, 1, 3, 2, 4, 1, 2])\n\ndef neg_log_l(lam, x):\n    if lam <= 0: return 1e10\n    n = len(x)\n    # log(L) = sum( k*log(lam) - lam - log(k!) - log(1 - exp(-lam)) )\n    term1 = np.sum(x * np.log(lam))\n    term2 = -n * lam\n    term3 = -np.sum(np.log(factorial(x)))\n    term4 = -n * np.log(1 - np.exp(-lam))\n    return -(term1 + term2 + term3 + term4)\n\nres = optimize.minimize_scalar(neg_log_l, args=(data,), bounds=(0.01, 10), method='bounded')\nprint(f\"MLE for lambda: {res.x:.4f}\")\n\n```\n\n---\n\n## Category: Concentration of Measure\n\n### Problem 14: Comparing Concentration Bounds\n\n**Description:**\nSuppose $X_1, \\dots, X_n$ are i.i.d. random variables with $E[X_i]=\\mu$ and $Var(X_i)=\\sigma^2$. You want to bound the probability $P(|\\bar{X}_n - \\mu| \\geq \\epsilon)$.\n\n**Tasks:**\n\n1. Which bound is generally tighter for large $n$: Chebyshev or Hoeffding? \n\n2. If $n=100$, $\\sigma^2=0.25$, and $\\epsilon=0.1$, calculate both bounds.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Parameters\nn = 100\nepsilon = 0.1\nmu = 0.5\n# For Uniform(0,1) or similar bounded [0,1], max variance is 0.25 (Bernoulli(0.5))\nvar = 0.25 \n\n# 1. Chebyshev: P(|X_bar - mu| >= eps) <= Var(X) / (n * eps^2)\ncheb_bound = var / (n * epsilon**2)\n\n# 2. Hoeffding: P(X_bar - mu >= eps) <= exp(-2 * n * eps^2)\n# (Note: Hoeffding handles one side, Chebyshev usually two sides. \n# For comparison we look at the one-sided versions if possible)\nhoeff_bound = np.exp(-2 * n * epsilon**2)\n\nprint(f\"Chebyshev Bound (upper limit): {cheb_bound:.4f}\")\nprint(f\"Hoeffding Bound: {hoeff_bound:.4f}\")\n# [cite_start]Hoeffding is significantly tighter (exponential decay vs polynomial)[cite: 14].\n\n```\n\n---\n\n## Category: Sampling\n\n### Problem 15: Rejection Sampling and Integration\n\n**Description:**\nGenerate 50,000 samples from the PDF $f(x) = \\frac{4}{\\pi} \\sqrt{1-x^2}$ for $x \\in [0,1]$ using a Uniform(0,1) proposal. Then, use these samples to estimate:\n$$ \\int_0^1 x^2 f(x) dx $$\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\ndef f(x):\n    return (4/np.pi) * np.sqrt(1 - x**2)\n\n# M = max(f(x)) occurs at x=0 -> f(0) = 4/pi\nM = 4/np.pi\n\ndef rejection_sample(n):\n    samples = []\n    while len(samples) < n:\n        x_prop = np.random.uniform(0, 1)\n        u = np.random.uniform(0, 1)\n        if u <= f(x_prop) / M:\n            samples.append(x_prop)\n    return np.array(samples)\n\nsamples = rejection_sample(50000)\n\n# Estimate integral using Monte Carlo (Mean of h(X) where X ~ f)\nintegral_est = np.mean(samples**2)\nprint(f\"Estimated Integral: {integral_est:.4f}\")\n\n# Analytical solution check: (4/pi) * integral(x^2 * sqrt(1-x^2))\n# Using substitution x=sin(t), this leads to 1/4.\nprint(f\"Analytical value: 0.25\")\n\n```\n\n---\n\n## Category: Classification\n\n### Problem 16: Cost-Sensitive Thresholds in Medicine\n\n**Description:**\nA diagnostic test identifies a disease ($Y=1$).\nCosts:\n\n* **False Negative (FN)**: 500 (Missing a disease is very dangerous) \n* **False Positive (FP)**: 20 (Unnecessary follow-up)\n* **TP/TN**: 0\n\n**Task:**\nFind the optimal probability threshold that minimizes the expected cost.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Simulate test results\nn = 5000\ny_true = np.random.binomial(1, 0.1, n) # 10% disease prevalence\ny_prob = np.random.uniform(0, 1, n)\n# Improve y_prob for true cases\ny_prob[y_true == 1] = np.random.beta(4, 2, np.sum(y_true == 1))\ny_prob[y_true == 0] = np.random.beta(2, 4, np.sum(y_true == 0))\n\nthresholds = np.linspace(0, 1, 100)\ncosts = []\n\nfor t in thresholds:\n    y_pred = (y_prob >= t).astype(int)\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    total_cost = fp * 20 + fn * 500\n    costs.append(total_cost)\n\nbest_t = thresholds[np.argmin(costs)]\nmin_avg_cost = np.min(avg_costs)\n\nprint(f\"Optimal Threshold: {best_t:.4f}\")\n# The threshold should be very low to avoid high-cost FNs.\n\n```\n---\n\n# 1MS041 Tentamensförberedelse - Massproduktion av Kärnfrågor (Omgång 3)\n\nDetta dokument fokuserar på de mest återkommande koncepten i 1MS041: Markovkedjor, MLE, Sampling, Koncentrationsolikheter och Kostnadskänslig klassificering.\n\n---\n\n## Category: Markov Chains (Transition Estimation & Hitting Times)\n\n### Problem 17: Logistics and Storage Status\n**Description:**\nA warehouse can have status: **Full (0)**, **Half Full (1)**, **Critical (2)**, or **Empty (3)**. You have observed the following sequence of daily statuses:\n`X = [0, 0, 1, 1, 2, 3, 0, 1, 2, 2, 1, 0, 0, 1, 3, 0]`\n\n**Tasks:**\n1. Estimate the transition matrix $P$ from the data.\n2. Calculate the stationary distribution $\\pi$.\n3. Calculate analytically the expected time (number of days) to reach \"Empty (3)\" starting from \"Full (0)\".\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Estimate P\nX = [0, 0, 1, 1, 2, 3, 0, 1, 2, 2, 1, 0, 0, 1, 3, 0]\nn_states = 4\nP = np.zeros((n_states, n_states))\n\nfor i in range(len(X)-1):\n    P[X[i], X[i+1]] += 1\n\n# Normalize rows\nrow_sums = P.sum(axis=1)\n# Handle rows with zero sum (if they exist)\nP = np.divide(P, row_sums[:, np.newaxis], out=np.zeros_like(P), where=row_sums[:, np.newaxis]!=0)\n\nprint(\"Estimated P:\\n\", P)\n\n# 2. Stationary distribution\n# Solve pi(P - I) = 0\nA = P.T - np.eye(n_states)\nA[-1] = np.ones(n_states)\nb = np.zeros(n_states)\nb[-1] = 1\npi = np.linalg.solve(A, b)\nprint(\"Stationary Distribution:\", pi)\n\n# 3. Hitting Time to State 3 starting from 0\n# E[T_i] = 1 + sum_{j != target} P_ij * E[T_j]\n# For i in {0, 1, 2}, target = 3\n# (I - Q) * E = 1\nQ = P[:3, :3]\nE = np.linalg.solve(np.eye(3) - Q, np.ones(3))\nprint(f\"Expected steps from state 0 to state 3: {E[0]:.2f}\")\n\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n### Problem 18: MLE for Custom Gamma-like PDF\n\n**Description:**\nGiven independent observations $x_1, \\dots, x_n$ from a distribution with probability density function:\n$$ f(x; \\theta) = \\frac{\\theta^3 x^2 e^{-\\theta x}}{2}, \\quad x > 0, \\theta > 0 $$\n\n**Tasks:**\n\n1. Derive the analytical formula for $\\theta_{MLE}$. \n\n2. Implement a function that calculates this for `data = [0.5, 1.2, 0.8, 2.5, 1.1]`. \n\n**Answer:**\nLog-likelihood:\n\n```python\nimport numpy as np\n\ndef mle_custom_gamma(x):\n    n = len(x)\n    return 3 * n / np.sum(x)\n\ndata = np.array([0.5, 1.2, 0.8, 2.5, 1.1])\ntheta_hat = mle_custom_gamma(data)\nprint(f\"Analytical MLE theta: {theta_hat:.4f}\")\n\n# Numerical verification\nfrom scipy.optimize import minimize\ndef neg_log_l(theta, x):\n    if theta <= 0: return 1e10\n    return -np.sum(3*np.log(theta) + 2*np.log(x) - theta*x - np.log(2))\n\nres = minimize(neg_log_l, x0=[1.0], args=(data,))\nprint(f\"Numerical MLE theta: {res.x[0]:.4f}\")\n\n```\n\n---\n\n## Category: Sampling & Integration\n\n### Problem 19: Inversion Sampling for a \"Power Law\"\n\n**Description:**\nWe want to generate samples from $F(x) = x^4$ for $x \\in [0,1]$. \n\n**Tasks:**\n\n1. Find the inverse function $F^{-1}(u)$. \n\n2. Generate 100,000 samples. \n\n3. Use the samples to estimate $\\int_0^1 \\cos(x) f(x) dx$. \n\n**Answer:**\n$f(x) = F'(x) = 4x^3$.\n\n```python\nimport numpy as np\n\n# 1 & 2. Inversion Sampling\nn_samples = 100000\nu = np.random.uniform(0, 1, n_samples)\nsamples = u**(1/4) # F^-1(u)\n\n# 3. Monte Carlo Integration\n# Density f(x) = F'(x) = 4x^3. \n# The integral is E[cos(X)] where X ~ f(x)\nintegral_est = np.mean(np.cos(samples))\nprint(f\"Estimated Integral: {integral_est:.4f}\")\n\n# Analytical check (Integration by parts): sin(1) + 4cos(1) + ... approx 0.60\n\n```\n\n---\n\n## Category: Classification and Concentration\n\n### Problem 20: Optimal Threshold and Hoeffding for Cost\n\n**Description:**\nYou have a model to detect defective products.\n\n* Cost for False Positive (FP): 5 (unnecessary inspection)\n* Cost for False Negative (FN): 50 (defective product reaches customer)\n* TP/TN cost: 0\nYou have validation data with probabilities `p` and true labels `y`. \n\n**Tasks:**\n\n1. Find the threshold $t$ that minimizes the average cost per product. \n\n2. Calculate a 99% confidence interval for the expected cost using Hoeffding's inequality. \n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Simulate data\nnp.random.seed(42)\nn_val = 2000\ny_val = np.random.binomial(1, 0.1, n_val)\np_val = np.random.uniform(0, 1, n_val)\np_val[y_val == 1] = np.random.beta(5, 2, np.sum(y_val == 1))\np_val[y_val == 0] = np.random.beta(2, 5, np.sum(y_val == 0))\n\ndef get_cost_vector(y_true, p_pred, threshold):\n    y_pred = (p_pred >= threshold).astype(int)\n    # Cost per observation\n    costs = np.zeros(len(y_true))\n    costs[(y_pred == 1) & (y_true == 0)] = 5  # FP\n    costs[(y_pred == 0) & (y_true == 1)] = 50 # FN\n    return costs\n\n# 1. Optimize threshold\nthresholds = np.linspace(0, 1, 101)\navg_costs = [np.mean(get_cost_vector(y_val, p_val, t)) for t in thresholds]\nbest_t = thresholds[np.argmin(avg_costs)]\nmin_avg_cost = np.min(avg_costs)\n\nprint(f\"Optimal Threshold: {best_t}\")\nprint(f\"Min Average Cost: {min_avg_cost:.4f}\")\n\n# 2. Hoeffding CI for 99% confidence\n# Cost C is bounded between [0, 50]. \n# Hoeffding: P(|mean(C) - E[C]| >= epsilon) <= 2 * exp(-2 * n * epsilon^2 / (b-a)^2)\nalpha = 0.01\nn = n_val\na, b = 0, 50\nepsilon = np.sqrt(((b - a)**2 * np.log(2 / alpha)) / (2 * n))\n\nci = (max(0, min_avg_cost - epsilon), min_avg_cost + epsilon)\nprint(f\"99% Confidence Interval for expected cost: {ci}\")\n\n```\n\n---\n\n## Category: Probability (Bayes' Theorem)\n\n### Problem 21: Conditional Probability for \"Expert Knowledge\"\n\n**Description:**\nA student takes an exam with 15 questions (Yes/No). \nThe number of questions the student *actually knows* is $N$.\nFor questions they don't know, they guess (50% correct).\nLet $Y$ be the total number correct. \n\n**Tasks:**\n\n1. If the student got 12 correct ($Y=12$), what is the probability that they actually *knew* fewer than 10 questions ($N < 10$)? \n\n**Solution and Code:**\n\n```python\nfrom scipy.special import binom\nimport numpy as np\n\n# P(N=k)\ndef p_N(k):\n    return binom(15, k) * (0.7**k) * (0.3**(15-k))\n\n# P(Y=12 | N=k)\n# If you know k questions, you must guess correctly on (12-k) of the remaining (15-k)\ndef p_Y_given_N(y, k):\n    if k > y: return 0\n    needed_guesses = y - k\n    remaining_q = 15 - k\n    if needed_guesses > remaining_q: return 0\n    return binom(remaining_q, needed_guesses) * (0.5**remaining_q)\n\n# P(Y=12) = sum_k P(Y=12 | N=k) * P(N=k)\np_Y_12 = sum(p_Y_given_N(12, k) * p_N(k) for k in range(16))\n\n# P(N < 10 | Y=12) = sum_{k < 10} P(Y=12 | N=k) * P(N=k) / P(Y=12)\np_N_less_10_and_Y_12 = sum(p_Y_given_N(12, k) * p_N(k) for k in range(10))\n\nresult = p_N_less_10_and_Y_12 / p_Y_12\nprint(f\"P(N < 10 | Y = 12) = {result:.4f}\")\n\n```\n\n---\n\n## Category: Covariance and Data Analysis\n\n### Problem 22: Geometric Interpretation of Covariance\n\n**Description:**\nYou have two variables $X$ and $Y$. You are told that their covariance matrix is:\n\n$$\n\\begin{pmatrix}\n4 & 1.5 \\\\\n1.5 & 1\n\\end{pmatrix}\n$$\n\n**Tasks:**\n\n1. What is the correlation $\\rho_{XY}$? \n\n2. If we transform the data to $Z = 2X - 3Y$, what is the variance of $Z$? \n\n**Answer:**\n\n1. $\\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}$.\n2. $\\text{Var}(Z) = 4\\text{Var}(X) + 9\\text{Var}(Y) + 2\\cdot2\\cdot(-3)\\text{Cov}(X,Y)$.\n\n```python\nimport numpy as np\ncov_matrix = np.array([[4, 1.5], [1.5, 1]])\nvar_x = cov_matrix[0,0]\nvar_y = cov_matrix[1,1]\ncov_xy = cov_matrix[0,1]\n\ncorr = cov_xy / (np.sqrt(var_x) * np.sqrt(var_y))\nvar_z = (2**2)*var_x + ((-3)**2)*var_y + 2*2*(-3)*cov_xy\n\nprint(f\"Correlation: {corr}\")\nprint(f\"Variance of Z: {var_z}\")\n\n```\n\n# 1MS041 Exam Preparation – Massive Problem Set (English)\n\n> **Note:** This document contains a comprehensive set of practice problems and solutions designed to mirror the structure and complexity of 1MS041 exams and assignments.  \n> Citations: [1], [3], [4], [6], [7], [8], [9]\n\n---\n\n## Category: Markov Chains\n\n### Problem 23: Cloud Infrastructure States\n\n**Description:**  \nA cloud server can be in three states: **Active (0)**, **Maintenance (1)**, and **Rebooting (2)**.  \nTransition probabilities:\n- $P(0 \\to 0) = 0.9$, $P(0 \\to 1) = 0.08$, $P(0 \\to 2) = 0.02$\n- $P(1 \\to 0) = 0.7$, $P(1 \\to 1) = 0.2$, $P(1 \\to 2) = 0.1$\n- $P(2 \\to 0) = 1.0$, $P(2 \\to 1) = 0$, $P(2 \\to 2) = 0$\n\n**Tasks:**\n1. Compute the stationary distribution $\\pi$.\n2. If the server is in Maintenance, what is the probability it is Active after 2 hours?\n3. Calculate the expected hitting time to state 2 starting from state 0.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nP = np.array([\n    [0.9, 0.08, 0.02],\n    [0.7, 0.2, 0.1],\n    [1.0, 0, 0]\n])\n\n# Stationary Distribution\nA = P.T - np.eye(3)\nA[-1] = np.ones(3)\nb = np.array([0, 0, 1])\npi = np.linalg.solve(A, b)\nprint(f\"Stationary Distribution: {pi}\")\n\n# Probability (1 -> 0) after 2 steps\nP2 = np.linalg.matrix_power(P, 2)\nprint(f\"P(X_2 = 0 | X_0 = 1) = {P2[1, 0]:.4f}\")\n\n# Expected Hitting Time to Rebooting (2) from Active (0)\nQ = P[:2, :2]\nI = np.eye(2)\nhitting_times = np.linalg.solve(I - Q, np.ones(2))\nprint(f\"Expected steps to state 2 from state 0: {hitting_times[0]:.2f}\")\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n### Problem 24: MLE for a Custom Density\n\n**Description:**  \nIID samples from PDF:  \n$f(x; \\alpha) = \\alpha^2 x e^{-\\alpha x}, \\quad x > 0, \\alpha > 0$\n\n**Tasks:**\n1. Derive the log-likelihood function $\\ell(\\alpha)$.\n2. Find the analytical MLE $\\hat{\\alpha}$.\n3. Numerically estimate $\\hat{\\alpha}$ for $x = [0.5, 1.0, 1.5, 2.0]$.\n\n**Solution:**\n- $\\ell(\\alpha) = 2n \\log(\\alpha) + \\sum \\log(x_i) - \\alpha \\sum x_i$\n- $\\hat{\\alpha} = 2 / \\bar{x}$\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndata = np.array([0.5, 1.0, 1.5, 2.0])\nalpha_hat_analytical = 2 / np.mean(data)\nprint(f\"Analytical MLE: {alpha_hat_analytical:.4f}\")\n\ndef neg_log_l(alpha, x):\n    if alpha <= 0: return 1e10\n    return -(2 * len(x) * np.log(alpha) + np.sum(np.log(x)) - alpha * np.sum(x))\n\nres = minimize(neg_log_l, x0=[1.0], args=(data,))\nprint(f\"Numerical MLE: {res.x[0]:.4f}\")\n```\n\n---\n\n## Category: Rejection Sampling\n\n### Problem 25: Sampling from a Triangle Distribution\n\n**Description:**  \nSample 100,000 from $f(x) = 2x$ for $x \\in [0,1]$ using Uniform(0,1) proposal.\n\n**Tasks:**\n1. Determine the constant $M$.\n2. Implement rejection sampling.\n3. Approximate $E[e^X]$ using the samples.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nM = 2\ndef f(x): return 2 * x\n\ndef rejection_sampling(n):\n    samples = []\n    while len(samples) < n:\n        x_prop = np.random.uniform(0, 1)\n        u = np.random.uniform(0, 1)\n        if u <= f(x_prop) / M:\n            samples.append(x_prop)\n    return np.array(samples)\n\nsamples = rejection_sampling(100000)\nintegral_approx = np.mean(np.exp(samples))\nprint(f\"Approximate Integral: {integral_approx:.4f}\")\n```\n\n---\n\n## Category: Concentration of Measure\n\n### Problem 26: Hoeffding Bound for Mean Absolute Error\n\n**Description:**  \nRegression model on $n$ points, absolute error $E_i \\in [0,10]$, observed mean error (MAE) is 1.5.\n\n**Tasks:**\n1. Construct a 95% confidence interval for the true expected MAE using Hoeffding's inequality.\n2. How many samples $n$ are needed to ensure the interval width is less than 0.5?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nn = 500\na, b = 0, 10\nalpha = 0.05\nepsilon = np.sqrt(((b - a)**2 * np.log(2 / alpha)) / (2 * n))\nmae_emp = 1.5\nci = (max(0, mae_emp - epsilon), mae_emp + epsilon)\nprint(f\"95% CI for MAE: {ci}\")\n\nn_needed = ((b-a)**2 * np.log(2/alpha)) / (2 * 0.25**2)\nprint(f\"Samples needed: {int(np.ceil(n_needed))}\")\n```\n\n---\n\n## Category: Classification Performance\n\n### Problem 27: Precision-Recall under Class Imbalance\n\n**Description:**  \nDataset: 90% \"Negative\", 10% \"Positive\".  \nConfusion matrix:\n- TP = 80, FN = 20\n- FP = 100, TN = 800\n\n**Tasks:**\n1. Calculate Precision and Recall for the Positive class.\n2. Calculate F1-score.\n3. If the cost of a False Negative is 10x the cost of a False Positive, should we decrease the threshold?\n\n**Solution:**\n- Precision = $80 / (80 + 100) \\approx 0.444$\n- Recall = $80 / (80 + 20) = 0.8$\n- F1 = $2 \\cdot (0.444 \\cdot 0.8) / (0.444 + 0.8) \\approx 0.571$\n- Yes, decrease the threshold to reduce costly FNs.\n\n---\n\n### Problem 28: Expected Steps in a Random Walk\n\n**Description:**  \nParticle on states $\\{0,1,2,3\\}$.  \nFrom 1: to 0 (0.5), stays (0.2), to 2 (0.3).  \nFrom 2: to 1 (0.5), stays (0.2), to 3 (0.3).  \nStates 0 and 3 are absorbing.\n\n**Task:**  \nCalculate expected steps to reach 3 from state 1.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nP = np.array([\n    [1, 0, 0, 0],\n    [0.5, 0.2, 0.3, 0],\n    [0, 0.5, 0.2, 0.3],\n    [0, 0, 0, 1]\n])\n\nQ = P[1:3, 1:3]\nI = np.eye(2)\nN = np.linalg.inv(I - Q)\nexpected_steps = N.dot(np.ones(2))\nprint(f\"Expected steps to absorption from state 1: {expected_steps[0]:.2f}\")\nprint(f\"Expected steps to absorption from state 2: {expected_steps[1]:.2f}\")\n```\n\n---\n\n## Category: Concentration & VC Dimension\n\n### Problem 29: VC-Dimension Bounds\n\n**Description:**  \nHypothesis class $H$ has VC-dimension $d$.  \n$n$ training samples, training error $err_{train}$.\n\n**Tasks:**\n1. Bound for true error: $err_{train} + \\sqrt{ (d (\\log(2n/d) + 1) + \\log(4/\\alpha)) / n }$\n2. Test set of size $n_{test}$, error $err_{test}$, Hoeffding bound for true error?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nd = 3\nn = 1000\nerr_train = 0.02\nalpha = 0.05\npenalty = np.sqrt( (d * (np.log(2*n/d) + 1) + np.log(4/alpha)) / n )\nvc_bound = err_train + penalty\nprint(f\"VC True Error Bound: {vc_bound:.4f}\")\n\nn_test = 200\nerr_test = 0.03\nepsilon_hoeff = np.sqrt( np.log(2/alpha) / (2 * n_test) )\nhoeff_bound = err_test + epsilon_hoeff\nprint(f\"Test Set Hoeffding Bound: {hoeff_bound:.4f}\")\n```\n\n---\n\n## Category: Data Transformations\n\n### Problem 30: Wind Velocity Covariance\n\n**Description:**  \nWind direction $\\theta$ (degrees), speed $v$.  \nData: $[(90, 5), (180, 10), (270, 5)]$\n\n**Tasks:**\n1. Convert to Cartesian coordinates $(v_x, v_y)$ (use radians).\n2. Compute empirical covariance matrix of velocity vectors.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\ndata = [(90, 5), (180, 10), (270, 5)]\nvectors = []\nfor deg, v in data:\n    rad = np.radians(deg)\n    vectors.append([v * np.cos(rad), v * np.sin(rad)])\n\nV = np.array(vectors)\nprint(f\"Velocity vectors:\\n{V}\")\n\ncov_matrix = np.cov(V, rowvar=False, bias=True)\nprint(f\"Empirical Covariance Matrix:\\n{cov_matrix}\")\n```\n\n# 1MS041 Master Practice Set - Core Exam Patterns (English)\n\n[cite_start]This collection focuses on the specific \"Core Problems\" that appear repeatedly in the 1MS041 exams: Markov Chains (transitions and hitting times), Binomial Probability (student/exam logic), MLE (analytical and numerical), and Concentration Bounds (Hoeffding)[cite: 1, 7, 10, 11].\n\n---\n\n## Category: Markov Chains (Hitting Times & Transitions)\n\n### Problem 31: Website Navigation Analysis (Exam Pattern)\n**Description:**\nA user on a news site moves between: **Home (0)**, **Article (1)**, and **Subscription Page (2)**.\nThe transition matrix is estimated as:\n$$\nP = \\begin{pmatrix}\n0.4 & 0.5 & 0.1 \\\\\n0.3 & 0.6 & 0.1 \\\\\n0.0 & 0.0 & 1.0\n\\end{pmatrix}\n$$\n[cite_start]Note: The \"Subscription Page\" (2) is an absorbing state[cite: 10].\n\n**Tasks:**\n1. [cite_start]Calculate the expected number of steps until a user reaches the Subscription Page starting from Home[cite: 7].\n2. [cite_start]If a user starts at Home, what is the probability they are reading an Article after 2 steps[cite: 10]?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Transition Matrix\nP = np.array([\n    [0.4, 0.5, 0.1],\n    [0.3, 0.6, 0.1],\n    [0.0, 0.0, 1.0]\n])\n\n# 1. Hitting Time to State 2 (Absorbing)\nQ = P[0:2, 0:2]\nI = np.eye(2)\nN = np.linalg.inv(I - Q)\nexpected_steps = N.dot(np.ones(2))\nprint(f\"Expected steps from Home (0) to Subscription (2): {expected_steps[0]:.2f}\")\n\n# 2. Probability Home -> Article after 2 steps\nP2 = np.linalg.matrix_power(P, 2)\nprint(f\"P(X_2 = 1 | X_0 = 0) = {P2[0, 1]:.4f}\")\n```\n\n---\n\n## Category: Probability & Bayes (The \"Exam/Student\" Pattern)\n\n### Problem 32: Quality Inspection (Pattern: Assignment 1, Problem 4)\n\n**Description:**\nA factory produces batches. The number of defective items $N$. An inspector checks the batch. If they find $\\geq 2$ defects, the batch is rejected. However, the inspector only detects a defect with 80% probability. For healthy items, there is a 5% \"false alarm\" rate where the inspector thinks it's defective.\n\n**Tasks:**\n1. Compute the probability that a batch actually has $<2$ defects given that it was rejected ($Y \\geq 2$).\n\n**Solution and Code:**\n\n```python\nfrom scipy.special import binom\nimport numpy as np\n\nn_total = 10\np_N = lambda k: binom(n_total, k) * (0.2**k) * (0.8**(n_total-k))\n\n# P(Y >= 2 | N = k)\ndef p_rejected_given_N(k):\n    n_sim = 20000\n    tp = np.random.binomial(k, 0.8, n_sim)\n    fp = np.random.binomial(n_total - k, 0.05, n_sim)\n    y = tp + fp\n    return np.mean(y >= 2)\n\np_Y_ge_2 = sum(p_rejected_given_N(k) * p_N(k) for k in range(n_total + 1))\np_N_less_2_and_Y_ge_2 = sum(p_rejected_given_N(k) * p_N(k) for k in range(2))\nresult = p_N_less_2_and_Y_ge_2 / p_Y_ge_2\nprint(f\"P(N < 2 | Rejected) = {result:.4f}\")\n```\n\n---\n\n## Category: MLE (Analytical and Numerical)\n\n### Problem 33: MLE for Poisson (Pattern: Exam June 2023, Problem 3)\n\n**Description:**\nA healthcare organization models physician visits using a Poisson distribution where $Y_i \\sim \\text{Poisson}(\\lambda_i)$, $\\lambda_i = \\exp(X_i \\beta)$.\n\n**Tasks:**\n1. Derive the negative log-likelihood for $n$ observations.\n2. Implement the `loss` function for optimization.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nclass PoissonRegression:\n    def loss(self, coeffs, X, Y):\n        lam = np.exp(np.dot(X, coeffs))\n        log_l = np.sum(Y * np.dot(X, coeffs) - lam)\n        return -log_l\n\n# Test\nX = np.array([[1, 2], [1, 3], [1, 1]])\nY = np.array([5, 10, 2])\nmodel = PoissonRegression()\nprint(f\"Loss for [0.5, 0.2]: {model.loss(np.array([0.5, 0.2]), X, Y):.4f}\")\n```\n\n---\n\n## Category: Sampling & Monte Carlo Integration\n\n### Problem 34: Semicircle Distribution (Pattern: Exam Jan 2024, Problem 1)\n\n**Description:**\nGenerate 100,000 samples from the PDF $f(x) = \\frac{2}{\\pi} \\sqrt{1-x^2}$ for $x \\in [-1,1]$.\n\n**Tasks:**\n1. Use the samples to approximate $E[|X|]$.\n2. Provide a 95% confidence interval using Hoeffding's inequality.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\ndef sample_semicircle(n):\n    samples = []\n    while len(samples) < n:\n        x_prop = np.random.uniform(-1, 1)\n        u = np.random.uniform(0, 1)\n        f_val = (2/np.pi) * np.sqrt(1 - x_prop**2)\n        if u <= f_val / (2/np.pi):\n            samples.append(x_prop)\n    return np.array(samples)\n\nsamples = sample_semicircle(100000)\nh_samples = np.abs(samples)\nintegral_est = np.mean(h_samples)\n\nn = 100000\nepsilon = np.sqrt(np.log(2/0.05) / (2 * n))\nprint(f\"Integral Estimate: {integral_est:.4f}\")\nprint(f\"95% CI: [{integral_est - epsilon:.4f}, {integral_est + epsilon:.4f}]\")\n```\n\n---\n\n## General Strategy for 1MS041 Exams\n\nTo succeed in this course, follow these general approaches for recurring problem types:\n\n### 1. Markov Chain Problems\n\n* **Stationary Distribution**: Always check if $\\pi P = \\pi$. In Python, use `np.linalg.solve(P.T - np.eye(n).T, b)` where the last row of the system is replaced by the sum condition $\\sum \\pi_i = 1$.\n* **Hitting Times**: Identify absorbing vs. transient states. Use the fundamental matrix $N = (I - Q)^{-1}$ where $Q$ contains only transitions between non-absorbing states.\n\n### 2. Maximum Likelihood (MLE)\n\n* **Analytical**: Write the likelihood $L(\\theta)$, take $\\log L$, differentiate, and set to zero. Common distributions: Normal, Exponential, Poisson, and Rayleigh.\n* **Numerical**: Use `scipy.optimize.minimize`. **Critical**: Always add a small `epsilon` or bounds to prevent `log(0)` or `sqrt(negative)` errors.\n\n### 3. Sampling & Integration\n\n* **Inversion**: If the CDF $F(x)$ is easy to invert, use $F^{-1}(u)$.\n* **Rejection**: Find $M$ such that $f(x) \\leq M g(x)$. Usually, $g(x)$ is a Uniform distribution.\n* **Monte Carlo**: To estimate $E[h(X)]$, simply draw samples $X$ from $f(x)$ and compute the average $h(X)$.\n\n### 4. Concentration Bounds (Guarantees)\n\n* **Hoeffding**: Use this for **Bounded** random variables (e.g., Accuracy $A$, Cost $C$).\n* **Chebyshev**: Use if you only know the **Variance**.\n* **Bennett's/Bernstein**: Use if the **Variance** is very small to get a tighter interval.\n\n### 5. Classification & Costs\n\n* **Optimal Threshold**: Don't assume $0.5$ is best. If the cost of a False Negative (FN) is high, the optimal threshold will be much lower than $0.5$.\n* **Metrics**: Remember that Precision and Recall are class-specific. Precision for class 1 is $TP / (TP + FP)$.\n\n---\n\n# 1MS041 Advanced Practice Set - Pattern Recognition & Implementation (English)\n\n[cite_start]This set focuses on high-yield exam patterns derived from previous assessments[cite: 1, 10, 11].\n\n---\n\n## Category: Markov Chains & Expected Steps\n\n### Problem 35: The \"Glider\" Communication Model\n**Description:**\nA communication packet is transmitted. It can be in three states: **In Transit (0)**, **Corrupted (1)**, or **Delivered (2)**.\n- From **In Transit**: 70% stay in transit, 20% get corrupted, 10% are delivered.\n- From **Corrupted**: 50% are retransmitted (go to In Transit), 50% stay corrupted.\n- From **Delivered**: This is an absorbing state ($P_{22} = 1$).\n\n**Tasks:**\n1. [cite_start]Construct the transition matrix $P$[cite: 7, 11].\n2. [cite_start]Calculate the expected number of steps until a packet is Delivered, starting from \"",
      "type": "study-guide",
      "keywords": [
        "markov",
        "mle",
        "sampling",
        "hoeffding",
        "classification",
        "concentration",
        "rejection sampling",
        "logistic regression",
        "poisson",
        "stationary distribution"
      ]
    },
    {
      "id": "study-guide-p1",
      "title": "Problem 1: Customer Behavior on Website",
      "category": "Markov Chains",
      "content": "### Problem 1: Customer Behavior on Website\n**Description:**\nAn e-commerce website models user behavior with three states: **Browsing (B)**, **Cart (C)**, and **Purchased (P)**. The transition probabilities are as follows:\n* From **Browsing**: 60% stay, 30% go to Cart, 10% leave (we ignore leaving for this closed chain and normalize: 0.6, 0.3, 0.1 distributed over B, C, P for simplicity; let's assume a closed model where P returns to B).\n* Let's define the matrix $P$ exactly:\n    * $P_{B \\to B} = 0.5, P_{B \\to C} = 0.4, P_{B \\to P} = 0.1$\n    * $P_{C \\to B} = 0.3, P_{C \\to C} = 0.2, P_{C \\to P} = 0.5$\n    * $P_{P \\to B} = 0.8, P_{P \\to C} = 0.0, P_{P \\to P} = 0.2$\n\n**Tasks:**\n1. Define the transition matrix in Python.\n2. Compute the stationary distribution.\n3. If a user starts in \"Browsing\", what is the probability they are in \"Purchased\" after exactly 3 steps?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Definiera övergångsmatrisen\n# Ordning: [Browsing, Cart, Purchased]\nP = np.array([\n    [0.5, 0.4, 0.1],\n    [0.3, 0.2, 0.5],\n    [0.8, 0.0, 0.2]\n])\n\nprint(\"Transition Matrix P:\\n\", P)\n\n# 2. Calculate stationary distribution\n# Solve pi * P = pi, which is the same as (P.T - I) * pi = 0\n# We add the condition that the sum of pi is 1.\neig_vals, eig_vecs = np.linalg.eig(P.T)\n# Find the eigenvector corresponding to eigenvalue 1 (or closest to 1)\nstationary_idx = np.argmin(np.abs(eig_vals - 1.0))\nstationary_vec = np.real(eig_vecs[:, stationary_idx])\nstationary_dist = stationary_vec / np.sum(stationary_vec)\n\nprint(\"\\nStationary Distribution (pi):\", stationary_dist)\n# Expected result (approximately): [0.48, 0.23, 0.29]\n\n# 3. Probability of being in Purchased after 3 steps starting from Browsing\n# Start vector (1, 0, 0)\nstart_state = np.array([1, 0, 0])\n# P after 3 steps is P^3\nP_3 = np.linalg.matrix_power(P, 3)\nprob_after_3 = np.dot(start_state, P_3)\n\nprint(\"\\nProbability distribution after 3 steps:\", prob_after_3)\nprint(\"Probability of being in 'Purchased' af",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p2",
      "title": "Problem 2: Estimation of Rayleigh Distribution",
      "category": "Maximum Likelihood Estimation (MLE)",
      "content": "### Problem 2: Estimation of Rayleigh Distribution\n\n**Description:**\nYou have collected data that is assumed to follow a Rayleigh distribution with probability density function:\n$$ f(x; \\sigma) = \\frac{x}{\\sigma^2} e^{-x^2 / (2\\sigma^2)}, \\quad x \\geq 0 $$\nYou have 50 observations. Write a function to numerically find the MLE for the parameter $\\sigma$.\n\n**Tasks:**\n\n1. Generate synthetic data (True $\\sigma$).\n2. Define the negative log-likelihood function.\n3. Minimize the function to find $\\sigma$.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\n\n# 1. Generate data\nnp.random.seed(42)\ntrue_sigma = 2.5\n# Rayleigh in numpy uses the 'scale' parameter as sigma\ndata = np.random.rayleigh(scale=true_sigma, size=50)\n\n# 2. Define negative log-likelihood\ndef neg_log_likelihood(params, x):\n    sigma = params[0]\n    if sigma <= 0: return np.inf # Constraint\n    \n    n = len(x)\n    # Log-likelihood L = sum(ln(x) - 2ln(sigma) - x^2/(2sigma^2))\n    # We can ignore sum(ln(x)) when minimizing since it doesn't depend on sigma, but we include it for completeness.\n    log_l = np.sum(np.log(x) - 2*np.log(sigma) - (x**2)/(2*sigma**2))\n    return -log_l\n\n# 3. Optimize\ninitial_guess = [1.0]\nresult = optimize.minimize(\n    neg_log_likelihood, \n    initial_guess, \n    args=(data,), \n    bounds=[(0.01, None)], # Sigma must be positive\n    method='L-BFGS-B'\n)\n\nestimated_sigma = result.x[0]\nprint(f\"True sigma: {true_sigma}\")\nprint(f\"Estimated sigma: {estimated_sigma:.4f}\")\n\n# Analytical solution for Rayleigh is sigma_hat = sqrt( sum(x^2) / (2N) )\nanalytical_sigma = np.sqrt(np.sum(data**2) / (2 * len(data)))\nprint(f\"Analytical check: {analytical_sigma:.4f}\")\n\n```\n\n---\n\n## Category: Sampling & Monte Carlo\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p3",
      "title": "Problem 3: Accept-Reject Sampling",
      "category": "Sampling & Monte Carlo",
      "content": "### Problem 3: Accept-Reject Sampling\n\n**Description:**\nWe want to sample from the distribution $f(x) = 3x^2$ for $x \\in [0,1]$.\nUse a Uniform(0,1) distribution as the proposal distribution $g(x)$.\n\n**Tasks:**\n\n1. Determine the constant $M$ so that $f(x) \\leq M g(x)$ for all $x$.\n2. Implement an `accept_reject` function that generates 10,000 samples.\n3. Calculate the integral $E[X]$ (i.e., the expected value $E[X]$) using Monte Carlo integration based on your samples.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Determine M\n# f(x) = 3x^2 on [0,1]. Maximum is at x=1, where f(1)=3.\n# g(x) = 1.\n# We must have 3x^2 <= M * 1. M = 3 is the minimum possible value.\nM = 3\n\ndef target_f(x):\n    return 3 * x**2\n\ndef accept_reject(n_samples):\n    samples = []\n    while len(samples) < n_samples:\n        # Generate proposal from Uniform(0,1)\n        x_prop = np.random.uniform(0, 1)\n        # Generate u from Uniform(0,1)\n        u = np.random.uniform(0, 1)\n        \n        # Acceptance criterion: u <= f(x) / (M * g(x))\n        if u <= target_f(x_prop) / (M * 1):\n            samples.append(x_prop)\n            \n    return np.array(samples)\n\n# 2. Generate samples\ngenerated_samples = accept_reject(10000)\n\n# Visualize (optional but good for verification)\n# plt.hist(generated_samples, bins=50, density=True, alpha=0.6, label='Samples')\n# xx = np.linspace(0,1,100)\n# plt.plot(xx, target_f(xx), 'r', label='True PDF')\n# plt.show()\n\n# 3. Monte Carlo Integration for E[X]\n# Integral x * f(x) dx is approximated by mean(samples) since samples are drawn from f(x).\nmonte_carlo_mean = np.mean(generated_samples)\ntrue_mean = 0.75 # Integral of x * 3x^2 = 3x^3 -> [3x^4/4]0..1 = 3/4\n\nprint(f\"Monte Carlo Estimated E[X]: {monte_carlo_mean:.4f}\")\nprint(f\"True E[X]: {true_mean}\")\n\n```\n\n---\n\n## Category: Classification and Confidence Intervals\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p4",
      "title": "Problem 4: Cost-Sensitive Classification and Hoeffding",
      "category": "Classification and Confidence Intervals",
      "content": "### Problem 4: Cost-Sensitive Classification and Hoeffding\n\n**Description:**\nYou have a model that predicts fraud (Fraud=1, Normal=0).\nCost matrix:\n\n* False Positive (FP): Cost 10 (Annoy customer)\n* False Negative (FN): Cost 100 (Lost money)\n* TP and TN: Cost 0\n\nYou have 1000 test points. Your model gives probabilities `y_proba`.\n\n**Tasks:**\n\n1. Write a function `calculate_cost(y_true, y_pred)` that calculates the total cost.\n2. Find the threshold (threshold) $t$ that minimizes cost on the test set.\n3. Calculate a 95% confidence interval for accuracy (accuracy) at this optimal threshold using Hoeffding's inequality.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Simulera data\nnp.random.seed(99)\nn_test = 1000\ny_true = np.random.binomial(1, 0.05, n_test) # 5% fraud\n# Simulera modell-sannolikheter (lite brusig men korrelerad)\ny_proba = np.random.uniform(0, 1, n_test)\ny_proba[y_true == 1] = np.random.beta(5, 1, np.sum(y_true == 1)) # Fraud har högre prob\ny_proba[y_true == 0] = np.random.beta(1, 5, np.sum(y_true == 0)) # Normal har lägre prob\n\n# 1. Kostnadsfunktion\ndef calculate_cost(y_true, y_pred):\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    cost = fp * 10 + fn * 100\n    return cost\n\n# 2. Hitta optimal threshold\nthresholds = np.linspace(0, 1, 101)\nbest_cost = float('inf')\nbest_t = 0.5\n\nfor t in thresholds:\n    y_pred_t = (y_proba >= t).astype(int)\n    current_cost = calculate_cost(y_true, y_pred_t)\n    if current_cost < best_cost:\n        best_cost = current_cost\n        best_t = t\n\nprint(f\"Optimal Threshold: {best_t}\")\nprint(f\"Minimal Cost: {best_cost}\")\n\n# 3. Hoeffding Intervall för Accuracy\n# Välj optimala prediktioner\nbest_preds = (y_proba >= best_t).astype(int)\nacc = accuracy_score(y_true, best_preds)\nn = len(y_true)\nalpha = 0.05 # 95% konfidens -> 5% felrisk\n\n# Hoeffding epsilon: sqrt(ln(2/alpha) / (2n))\nepsilon = np.sqrt(np.log(2 / alpha) / (2 * n))\nci_lowe",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p5",
      "title": "Problem 5: Bag-of-Words and Probability",
      "category": "Text Analysis and Data Handling",
      "content": "### Problem 5: Bag-of-Words and Probability\n\n**Description:**\nGiven a list of SMS messages, calculate the conditional probability $P(Spam | \\text{'win' in text})$.\nUse `CountVectorizer` to identify the word.\n\n**Tasks:**\n\n1. Prepare the data.\n2. Create a binary vector for whether the word \"win\" exists in each text.\n3. Calculate the empirical probability.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Exempeldata\ntexts = [\n    \"Win a free prize now\",\n    \"Meeting at noon\",\n    \"You have won a lottery win\",\n    \"Can we talk later?\",\n    \"Win big money\",\n    \"Project deadline tomorrow\"\n]\n# 1 = Spam, 0 = Ham\nlabels = np.array([1, 0, 1, 0, 1, 0])\n\n# 1. Vectorizer (binary=True för existens snarare än antal)\nvectorizer = CountVectorizer(binary=True)\nX = vectorizer.fit_transform(texts)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Hitta index för ordet \"win\"\ntry:\n    win_index = np.where(feature_names == \"win\")[0][0]\nexcept IndexError:\n    print(\"Ordet 'win' finns inte i vokabulären.\")\n    win_index = None\n\nif win_index is not None:\n    # 2. Hitta vilka texter som innehåller \"win\"\n    # X är en sparse matrix, hämta kolumnen för \"win\"\n    has_win = X[:, win_index].toarray().flatten()\n    \n    # 3. Beräkna P(Spam | \"win\")\n    # P(A|B) = P(A och B) / P(B) -> Antal(Spam och Win) / Antal(Win)\n    num_win = np.sum(has_win)\n    num_spam_and_win = np.sum(labels[has_win == 1])\n    \n    if num_win > 0:\n        p_spam_given_win = num_spam_and_win / num_win\n        print(f\"Antal texter med 'win': {num_win}\")\n        print(f\"Antal av dessa som är spam: {num_spam_and_win}\")\n        print(f\"P(Spam | 'win' in text) = {p_spam_given_win:.4f}\")\n    else:\n        print(\"Inga texter innehöll ordet 'win'.\")\n\n```\n\n---\n\n## Category: Concentration of Measure\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p6",
      "title": "Problem 6: Comparison of Concentrations",
      "category": "Concentration of Measure",
      "content": "### Problem 6: Comparison of Concentrations\n\n**Description:**\nWhich of the following concentrates **exponentially** quickly toward its expected value as the number of samples $n$ increases?\n\n1. The empirical mean of i.i.d. variables with finite variance (but not bounded)?\n2. The empirical mean of i.i.d. bounded random variables (Bounded RVs)?\n3. The empirical mean of a Cauchy distribution?\n\n**Answer:**\n\n* **Option 2** concentrates exponentially. This is the core of Hoeffding's inequality ($P(|\\bar{X}_n - \\mu| > \\epsilon) \\leq 2e^{-2n\\epsilon^2/(b-a)^2}$).\n* Option 1 usually concentrates polynomially (via Chebyshev's inequality) if we only assume finite variance without stronger assumptions (like sub-Gaussian).\n* Option 3 (Cauchy) has no expected value and does not concentrate at all (Law of large numbers does not apply).\n\n**Code Example for Verification (Simulation):**\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN_experiments = 1000\nsample_sizes = [10, 100, 500, 1000]\nthreshold = 0.1\n\nprint(\"Probability that deviation > 0.1 for different n (Bounded vs Pareto):\")\n\nfor n in sample_sizes:\n    # Bounded (Uniform 0,1), Mean = 0.5\n    bounded_means = np.mean(np.random.uniform(0, 1, (N_experiments, n)), axis=1)\n    prob_bounded = np.mean(np.abs(bounded_means - 0.5) > threshold)\n    \n    # Heavy tail (Pareto, a=1.5), Mean exists but variance infinite/large\n    # Pareto mean = a / (a-1) = 3.0\n    # We use standard t-distribution with df=3 for \"finite variance but not bounded\"\n    # Mean = 0\n    heavy_means = np.mean(np.random.standard_t(df=3, size=(N_experiments, n)), axis=1)\n    prob_heavy = np.mean(np.abs(heavy_means - 0) > threshold)\n    \n    print(f\"n={n}: Bounded Prob={prob_bounded:.4f}, T-dist Prob={prob_heavy:.4f}\")\n\n# You can clearly see that Bounded Prob goes to 0 much faster than T-dist Prob.\n\n```\n\n# 1MS041 Exam Preparation - Round 2\n\nHere are additional newly created variants of exercises based on course material, with focus on Probability Theor",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p7",
      "title": "Problem 7: Quality Control in Factory (Binomial Distribution)",
      "category": "Probability and Conditioning",
      "content": "### Problem 7: Quality Control in Factory (Binomial Distribution)\n**Description:**\nA factory produces batches of 50 components. The number of defective components in a batch, $N$, is assumed to follow a binomial distribution $N \\sim \\text{Bin}(50, 0.05)$.\nThe factory has an automatic testing system that raises an alarm (discards the batch) if the number of detected defects $Y \\ge T$.\nHowever, the system is not perfect. If a component is defective, it is detected with 90% probability. If a component is intact, it is falsely marked as defective with 1% probability.\nLet $Y$ be the number of *reported* defects.\n\n**Tasks:**\n1. Simulate the process for 100,000 batches to estimate the distribution of $Y$.\n2. Set the threshold $T=5$. Calculate the conditional probability that a batch actually has fewer than 2 defective components ($N < 2$) given that the system raised an alarm ($Y \\ge 5$).\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Parameters\nn_items = 50\np_defect = 0.05\np_detect_given_defect = 0.90\np_alarm_given_healthy = 0.01\nn_sim = 100000\nT = 5\n\n# 1. Simulation\n# N: Number of actually defective components in each batch\nN = np.random.binomial(n_items, p_defect, n_sim)\n\n# Y: Number of reported defects\n# Y consists of detected defectives (True Positives) + false positives (False Positives)\n# Number of intact = n_items - N\nTP = np.random.binomial(N, p_detect_given_defect)\nFP = np.random.binomial(n_items - N, p_alarm_given_healthy)\nY = TP + FP\n\n# 2. Conditional probability P(N < 2 | Y >= T)\n# Filter out cases where the alarm went off\nalarm_indices = Y >= T\nN_given_alarm = N[alarm_indices]\n\n# Calculate the fraction where N < 2\nprob_N_less_2_given_alarm = np.mean(N_given_alarm < 2)\n\nprint(f\"Estimated P(N < 2 | Y >= {T}) = {prob_N_less_2_given_alarm:.4f}\")\n\n```\n\n---\n\n## Category: Data Analysis and Linear Algebra\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p8",
      "title": "Problem 8: Motion Analysis and Covariance",
      "category": "Data Analysis and Linear Algebra",
      "content": "### Problem 8: Motion Analysis and Covariance\n\n**Description:**\nYou have data about a robot's position at 100 time points. The data is in variables `x_pos` and `y_pos`.\nYou should analyze the robot's motion.\n\n**Tasks:**\n\n1. Create a numpy array `positions` of size (2, 100).\n2. Calculate the **mean position** (centroid).\n3. Calculate the **empirical covariance matrix** for the positions (should be 2x2).\n4. Calculate the distance from each point to the mean position and state the average distance.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Generate synthetic data (correlated motion)\nnp.random.seed(42)\nx_pos = np.random.normal(10, 2, 100)\ny_pos = 0.5 * x_pos + np.random.normal(0, 1, 100)\n\n# 1. Create array\npositions = np.vstack((x_pos, y_pos))\nprint(f\"Shape: {positions.shape}\") # Should be (2, 100)\n\n# 2. Mean position\nmean_pos = np.mean(positions, axis=1)\nprint(f\"Mean position (x, y): {mean_pos}\")\n\n# 3. Covariance matrix\n# bias=True for empirical covariance (divided by N), bias=False for N-1 (more common in statistics)\n# The task says \"empirical\", often 1/N, but np.cov defaults to 1/(N-1). We use standard np.cov.\ncov_matrix = np.cov(positions)\nprint(\"Covariance matrix:\\n\", cov_matrix)\n\n# 4. Average distance to mean position\n# Center the data\ncentered_pos = positions.T - mean_pos\n# Euclidean distance for each point (norm of vectors)\ndistances = np.linalg.norm(centered_pos, axis=1)\navg_distance = np.mean(distances)\n\nprint(f\"Average distance to centroid: {avg_distance:.4f}\")\n\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p9",
      "title": "Problem 9: MLE for Exponential Distribution",
      "category": "Maximum Likelihood Estimation (MLE)",
      "content": "### Problem 9: MLE for Exponential Distribution\n\n**Description:**\nThe time between arrivals to a server is assumed to follow an exponential distribution with probability density function:\n$$ f(x; \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0 $$\nYou have a list `arrival_times` with $n$ observations.\n\n**Tasks:**\n\n1. Derive (on paper/theoretically) the MLE for $\\lambda$. (Answer: $\\lambda_{MLE} = 1/\\bar{x}$).\n2. Write a function `mle_exponential(data)` that returns the estimate given the data.\n3. Use `scipy.optimize.minimize` to numerically find $\\lambda$ by minimizing negative log-likelihood and verify that it matches the analytical solution.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\n\n# Synthetic data (True lambda = 0.5)\ntrue_lambda = 0.5\ndata = np.random.exponential(1/true_lambda, 1000)\n\n# 1 & 2. Analytical solution\ndef mle_exponential(data):\n    # lambda_hat = 1 / mean(x)\n    return 1.0 / np.mean(data)\n\nanalytical_est = mle_exponential(data)\nprint(f\"Analytical estimate: {analytical_est:.4f}\")\n\n# 3. Numerical solution\ndef neg_log_likelihood(params, x):\n    lam = params[0]\n    if lam <= 0: return np.inf\n    # L = n*ln(lambda) - lambda * sum(x)\n    n = len(x)\n    log_l = n * np.log(lam) - lam * np.sum(x)\n    return -log_l\n\nres = optimize.minimize(\n    neg_log_likelihood, \n    x0=[1.0], \n    args=(data,), \n    bounds=[(0.001, None)]\n)\nnumerical_est = res.x[0]\n\nprint(f\"Numerical estimate:  {numerical_est:.4f}\")\nprint(f\"Difference: {abs(analytical_est - numerical_est):.2e}\")\n\n```\n\n---\n\n## Category: Text Analysis and Confidence Intervals\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p10",
      "title": "Problem 10: Probability Estimation in SMS Data",
      "category": "Text Analysis and Confidence Intervals",
      "content": "### Problem 10: Probability Estimation in SMS Data\n\n**Description:**\nYou have a large collection of SMS data classified as Spam (1) or Not Spam (0).\nYou want to estimate the probability $P(Spam | \\text{'call' in text})$.\n\n**Tasks:**\n\n1. Use `CountVectorizer` to create a matrix of word occurrences.\n2. Calculate the point estimate $\\hat{p}$ for the above probability.\n3. Calculate a 95% confidence interval for this probability using **Hoeffding's inequality**.\n*Remember:* Hoeffding's interval for a mean $\\hat{p}$ is $[\\hat{p} - \\epsilon, \\hat{p} + \\epsilon]$ where $\\epsilon = \\sqrt{\\ln(2/\\alpha)/(2n)}$. Here $n$ is the number of SMS containing the word \"call\".\n\n**Solution and Code:**\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Example data\nsms_corpus = [\n    \"Call me later\", \n    \"You won a prize call now\", \n    \"Call for free money\", \n    \"Meeting at 10\", \n    \"Please call back\",\n    \"URGENT call now\"\n]\n# 1 = Spam, 0 = Ham\ny = np.array([0, 1, 1, 0, 0, 1])\n\n# 1. Vectorizer\nvec = CountVectorizer(binary=True) # binary=True because we only care if the word exists\nX = vec.fit_transform(sms_corpus)\nfeat_names = vec.get_feature_names_out()\n\ntarget_word = \"call\"\nif target_word in feat_names:\n    idx = np.where(feat_names == target_word)[0][0]\n    \n    # Find which documents contain the word\n    has_word = X[:, idx].toarray().flatten() == 1\n    \n    # Filter y based on these documents\n    y_subset = y[has_word]\n    n = len(y_subset)\n    \n    if n > 0:\n        # 2. Point estimate\n        p_hat = np.mean(y_subset)\n        \n        # 3. Hoeffding's interval\n        alpha = 0.05\n        epsilon = np.sqrt(np.log(2/alpha) / (2 * n))\n        \n        ci_lower = max(0, p_hat - epsilon)\n        ci_upper = min(1, p_hat + epsilon)\n        \n        print(f\"The word '{target_word}' was found in {n} messages.\")\n        print(f\"Point estimate p_hat: {p_hat:.4f}\")\n        print(f\"95% CI (Hoeffding): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n    else:\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p11",
      "title": "Problem 11: Implement Loss Function for Logistic Regression",
      "category": "Optimization and Machine Learning",
      "content": "### Problem 11: Implement Loss Function for Logistic Regression\n\n**Description:**\nIn the course, a \"Proportional Model\" (Logistic Regression) is often used where $\\hat{y}_i = \\sigma(\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip})$.\nTo train this model, we need to minimize the negative log-likelihood (Loss function).\n\n**Tasks:**\n\n1. Create a class `LogisticModel`.\n2. Implement the method `loss(coeffs, X, Y)`.\n* `coeffs`: Array where `coeffs[0]` is the intercept ($\\beta_0$) and the rest are weights ($\\beta_1, ...$).\n* The loss function for $N$ observations is:\n$$ J(\\beta) = - \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right] $$\nwhere $\\hat{y}_i = \\sigma(\\beta_0 + \\beta_1 x_{i1} + ...)$.\n\n3. Add a small regularization (Ridge/L2) to the loss function: $\\lambda \\sum_j \\beta_j^2$ (exclude the intercept if you want to be precise, but here we can include all for simplicity). $\\lambda = 0.1$.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nclass LogisticModel:\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def loss(self, coeffs, X, Y):\n        # coeffs[0] is intercept, coeffs[1:] are feature weights\n        intercept = coeffs[0]\n        beta = coeffs[1:]\n        \n        # Calculate linear combination z = beta*x + beta0\n        z = np.dot(X, beta) + intercept\n        \n        # Prediction (probability)\n        y_pred = self.sigmoid(z)\n        \n        # Avoid log(0) by clipping values\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        \n        # Negative Log Likelihood\n        # Formula: -sum(y*log(p) + (1-y)*log(1-p))\n        nll = -np.sum(Y * np.log(y_pred) + (1 - Y) * np.log(1 - y_pred))\n        \n        # L2 Regularization (lambda * sum(weights^2))\n        l2_lambda = 0.1\n        reg_term = l2_lambda * np.sum(coeffs**2)\n        \n        return nll + reg_term\n\n# Test of the function\nX_dummy = np.array([[1, 2], [2, 1], [0, 0]])\nY_dummy = np.array([1, 0, 0])\n# Guessed coefficients ",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p12",
      "title": "Problem 12: Network Server Reliability",
      "category": "Markov Chains",
      "content": "### Problem 12: Network Server Reliability\n**Description:**\nA server can be in one of three states: **Operational (O)**, **Degraded (D)**, or **Failed (F)**. The transition probabilities per hour are:\n- From **Operational**: 80% stay Operational, 15% become Degraded, 5% Fail.\n- From **Degraded**: 0% become Operational (needs repair), 70% stay Degraded, 30% Fail.\n- From **Failed**: 100% become Operational after repair (takes exactly one hour).\n\n**Tasks:**\n1. [cite_start]Define the transition matrix $P$[cite: 10, 17].\n2. [cite_start]Determine if the chain is irreducible and aperiodic[cite: 18].\n3. [cite_start]Calculate the stationary distribution[cite: 14, 17].\n4. [cite_start]What is the expected number of hours until the server first fails, starting from Operational? [cite: 17, 10]\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Transition Matrix (States: O, D, F)\nP = np.array([\n    [0.80, 0.15, 0.05],\n    [0.00, 0.70, 0.30],\n    [1.00, 0.00, 0.00]\n])\n\n# 2. Properties\n# Irreducible: Yes, possible to reach any state from any state (O->D->F->O).\n# Aperiodic: Yes, P[0,0] > 0 (self-loop).\n\n# 3. Stationary Distribution\n# Solve pi * P = pi\nevals, evecs = np.linalg.eig(P.T)\npi = np.real(evecs[:, np.isclose(evals, 1)])\npi = pi / pi.sum()\nprint(f\"Stationary Distribution: {pi.flatten()}\")\n\n# 4. Expected Hitting Time to F starting from O\n# Solve system: E[T_O] = 1 + 0.8*E[T_O] + 0.15*E[T_D]\n#               E[T_D] = 1 + 0.7*E[T_D]\n# (Note: E[T_F] = 0)\n# From 2nd eq: 0.3*E[T_D] = 1 => E[T_D] = 10/3\n# Substitute into 1st: 0.2*E[T_O] = 1 + 0.15*(10/3) = 1 + 0.5 = 1.5\n# E[T_O] = 1.5 / 0.2 = 7.5 hours.\n\n# Matrix approach for confirmation\nQ = P[:2, :2] # Submatrix excluding state F\nI = np.eye(2)\nN = np.linalg.inv(I - Q) # Fundamental matrix\nhitting_times = N.dot(np.ones(2))\nprint(f\"Expected steps to hit F: from O={hitting_times[0]:.2f}, from D={hitting_times[1]:.2f}\")\n\n```\n\n---\n\n## Category: Maximum Likelihood Estimation\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p13",
      "title": "Problem 13: MLE for Zero-Truncated Poisson",
      "category": "Maximum Likelihood Estimation",
      "content": "### Problem 13: MLE for Zero-Truncated Poisson\n\n**Description:**\nIn some scenarios, we only observe counts greater than zero (e.g., number of items bought by a customer who actually entered the store). This follows a Zero-Truncated Poisson distribution:\n$$ P(X=k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k! (1 - e^{-\\lambda})}, \\quad k = 1, 2, \\dots $$\n\n**Tasks:**\n\n1. Implement the negative log-likelihood function.\n\n2. Numerically find the MLE $\\lambda$ for the dataset `data = [1, 2, 1, 3, 2, 4, 1, 2]`.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import factorial\n\ndata = np.array([1, 2, 1, 3, 2, 4, 1, 2])\n\ndef neg_log_l(lam, x):\n    if lam <= 0: return 1e10\n    n = len(x)\n    # log(L) = sum( k*log(lam) - lam - log(k!) - log(1 - exp(-lam)) )\n    term1 = np.sum(x * np.log(lam))\n    term2 = -n * lam\n    term3 = -np.sum(np.log(factorial(x)))\n    term4 = -n * np.log(1 - np.exp(-lam))\n    return -(term1 + term2 + term3 + term4)\n\nres = optimize.minimize_scalar(neg_log_l, args=(data,), bounds=(0.01, 10), method='bounded')\nprint(f\"MLE for lambda: {res.x:.4f}\")\n\n```\n\n---\n\n## Category: Concentration of Measure\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p14",
      "title": "Problem 14: Comparing Concentration Bounds",
      "category": "Concentration of Measure",
      "content": "### Problem 14: Comparing Concentration Bounds\n\n**Description:**\nSuppose $X_1, \\dots, X_n$ are i.i.d. random variables with $E[X_i]=\\mu$ and $Var(X_i)=\\sigma^2$. You want to bound the probability $P(|\\bar{X}_n - \\mu| \\geq \\epsilon)$.\n\n**Tasks:**\n\n1. Which bound is generally tighter for large $n$: Chebyshev or Hoeffding? \n\n2. If $n=100$, $\\sigma^2=0.25$, and $\\epsilon=0.1$, calculate both bounds.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Parameters\nn = 100\nepsilon = 0.1\nmu = 0.5\n# For Uniform(0,1) or similar bounded [0,1], max variance is 0.25 (Bernoulli(0.5))\nvar = 0.25 \n\n# 1. Chebyshev: P(|X_bar - mu| >= eps) <= Var(X) / (n * eps^2)\ncheb_bound = var / (n * epsilon**2)\n\n# 2. Hoeffding: P(X_bar - mu >= eps) <= exp(-2 * n * eps^2)\n# (Note: Hoeffding handles one side, Chebyshev usually two sides. \n# For comparison we look at the one-sided versions if possible)\nhoeff_bound = np.exp(-2 * n * epsilon**2)\n\nprint(f\"Chebyshev Bound (upper limit): {cheb_bound:.4f}\")\nprint(f\"Hoeffding Bound: {hoeff_bound:.4f}\")\n# [cite_start]Hoeffding is significantly tighter (exponential decay vs polynomial)[cite: 14].\n\n```\n\n---\n\n## Category: Sampling\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p15",
      "title": "Problem 15: Rejection Sampling and Integration",
      "category": "Sampling",
      "content": "### Problem 15: Rejection Sampling and Integration\n\n**Description:**\nGenerate 50,000 samples from the PDF $f(x) = \\frac{4}{\\pi} \\sqrt{1-x^2}$ for $x \\in [0,1]$ using a Uniform(0,1) proposal. Then, use these samples to estimate:\n$$ \\int_0^1 x^2 f(x) dx $$\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\ndef f(x):\n    return (4/np.pi) * np.sqrt(1 - x**2)\n\n# M = max(f(x)) occurs at x=0 -> f(0) = 4/pi\nM = 4/np.pi\n\ndef rejection_sample(n):\n    samples = []\n    while len(samples) < n:\n        x_prop = np.random.uniform(0, 1)\n        u = np.random.uniform(0, 1)\n        if u <= f(x_prop) / M:\n            samples.append(x_prop)\n    return np.array(samples)\n\nsamples = rejection_sample(50000)\n\n# Estimate integral using Monte Carlo (Mean of h(X) where X ~ f)\nintegral_est = np.mean(samples**2)\nprint(f\"Estimated Integral: {integral_est:.4f}\")\n\n# Analytical solution check: (4/pi) * integral(x^2 * sqrt(1-x^2))\n# Using substitution x=sin(t), this leads to 1/4.\nprint(f\"Analytical value: 0.25\")\n\n```\n\n---\n\n## Category: Classification\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p16",
      "title": "Problem 16: Cost-Sensitive Thresholds in Medicine",
      "category": "Classification",
      "content": "### Problem 16: Cost-Sensitive Thresholds in Medicine\n\n**Description:**\nA diagnostic test identifies a disease ($Y=1$).\nCosts:\n\n* **False Negative (FN)**: 500 (Missing a disease is very dangerous) \n* **False Positive (FP)**: 20 (Unnecessary follow-up)\n* **TP/TN**: 0\n\n**Task:**\nFind the optimal probability threshold that minimizes the expected cost.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Simulate test results\nn = 5000\ny_true = np.random.binomial(1, 0.1, n) # 10% disease prevalence\ny_prob = np.random.uniform(0, 1, n)\n# Improve y_prob for true cases\ny_prob[y_true == 1] = np.random.beta(4, 2, np.sum(y_true == 1))\ny_prob[y_true == 0] = np.random.beta(2, 4, np.sum(y_true == 0))\n\nthresholds = np.linspace(0, 1, 100)\ncosts = []\n\nfor t in thresholds:\n    y_pred = (y_prob >= t).astype(int)\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    total_cost = fp * 20 + fn * 500\n    costs.append(total_cost)\n\nbest_t = thresholds[np.argmin(costs)]\nmin_avg_cost = np.min(avg_costs)\n\nprint(f\"Optimal Threshold: {best_t:.4f}\")\n# The threshold should be very low to avoid high-cost FNs.\n\n```\n---\n\n# 1MS041 Tentamensförberedelse - Massproduktion av Kärnfrågor (Omgång 3)\n\nDetta dokument fokuserar på de mest återkommande koncepten i 1MS041: Markovkedjor, MLE, Sampling, Koncentrationsolikheter och Kostnadskänslig klassificering.\n\n---\n\n## Category: Markov Chains (Transition Estimation & Hitting Times)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p17",
      "title": "Problem 17: Logistics and Storage Status",
      "category": "Markov Chains (Transition Estimation & Hitting Times)",
      "content": "### Problem 17: Logistics and Storage Status\n**Description:**\nA warehouse can have status: **Full (0)**, **Half Full (1)**, **Critical (2)**, or **Empty (3)**. You have observed the following sequence of daily statuses:\n`X = [0, 0, 1, 1, 2, 3, 0, 1, 2, 2, 1, 0, 0, 1, 3, 0]`\n\n**Tasks:**\n1. Estimate the transition matrix $P$ from the data.\n2. Calculate the stationary distribution $\\pi$.\n3. Calculate analytically the expected time (number of days) to reach \"Empty (3)\" starting from \"Full (0)\".\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Estimate P\nX = [0, 0, 1, 1, 2, 3, 0, 1, 2, 2, 1, 0, 0, 1, 3, 0]\nn_states = 4\nP = np.zeros((n_states, n_states))\n\nfor i in range(len(X)-1):\n    P[X[i], X[i+1]] += 1\n\n# Normalize rows\nrow_sums = P.sum(axis=1)\n# Handle rows with zero sum (if they exist)\nP = np.divide(P, row_sums[:, np.newaxis], out=np.zeros_like(P), where=row_sums[:, np.newaxis]!=0)\n\nprint(\"Estimated P:\\n\", P)\n\n# 2. Stationary distribution\n# Solve pi(P - I) = 0\nA = P.T - np.eye(n_states)\nA[-1] = np.ones(n_states)\nb = np.zeros(n_states)\nb[-1] = 1\npi = np.linalg.solve(A, b)\nprint(\"Stationary Distribution:\", pi)\n\n# 3. Hitting Time to State 3 starting from 0\n# E[T_i] = 1 + sum_{j != target} P_ij * E[T_j]\n# For i in {0, 1, 2}, target = 3\n# (I - Q) * E = 1\nQ = P[:3, :3]\nE = np.linalg.solve(np.eye(3) - Q, np.ones(3))\nprint(f\"Expected steps from state 0 to state 3: {E[0]:.2f}\")\n\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p18",
      "title": "Problem 18: MLE for Custom Gamma-like PDF",
      "category": "Maximum Likelihood Estimation (MLE)",
      "content": "### Problem 18: MLE for Custom Gamma-like PDF\n\n**Description:**\nGiven independent observations $x_1, \\dots, x_n$ from a distribution with probability density function:\n$$ f(x; \\theta) = \\frac{\\theta^3 x^2 e^{-\\theta x}}{2}, \\quad x > 0, \\theta > 0 $$\n\n**Tasks:**\n\n1. Derive the analytical formula for $\\theta_{MLE}$. \n\n2. Implement a function that calculates this for `data = [0.5, 1.2, 0.8, 2.5, 1.1]`. \n\n**Answer:**\nLog-likelihood:\n\n```python\nimport numpy as np\n\ndef mle_custom_gamma(x):\n    n = len(x)\n    return 3 * n / np.sum(x)\n\ndata = np.array([0.5, 1.2, 0.8, 2.5, 1.1])\ntheta_hat = mle_custom_gamma(data)\nprint(f\"Analytical MLE theta: {theta_hat:.4f}\")\n\n# Numerical verification\nfrom scipy.optimize import minimize\ndef neg_log_l(theta, x):\n    if theta <= 0: return 1e10\n    return -np.sum(3*np.log(theta) + 2*np.log(x) - theta*x - np.log(2))\n\nres = minimize(neg_log_l, x0=[1.0], args=(data,))\nprint(f\"Numerical MLE theta: {res.x[0]:.4f}\")\n\n```\n\n---\n\n## Category: Sampling & Integration\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p19",
      "title": "Problem 19: Inversion Sampling for a \"Power Law\"",
      "category": "Sampling & Integration",
      "content": "### Problem 19: Inversion Sampling for a \"Power Law\"\n\n**Description:**\nWe want to generate samples from $F(x) = x^4$ for $x \\in [0,1]$. \n\n**Tasks:**\n\n1. Find the inverse function $F^{-1}(u)$. \n\n2. Generate 100,000 samples. \n\n3. Use the samples to estimate $\\int_0^1 \\cos(x) f(x) dx$. \n\n**Answer:**\n$f(x) = F'(x) = 4x^3$.\n\n```python\nimport numpy as np\n\n# 1 & 2. Inversion Sampling\nn_samples = 100000\nu = np.random.uniform(0, 1, n_samples)\nsamples = u**(1/4) # F^-1(u)\n\n# 3. Monte Carlo Integration\n# Density f(x) = F'(x) = 4x^3. \n# The integral is E[cos(X)] where X ~ f(x)\nintegral_est = np.mean(np.cos(samples))\nprint(f\"Estimated Integral: {integral_est:.4f}\")\n\n# Analytical check (Integration by parts): sin(1) + 4cos(1) + ... approx 0.60\n\n```\n\n---\n\n## Category: Classification and Concentration\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p20",
      "title": "Problem 20: Optimal Threshold and Hoeffding for Cost",
      "category": "Classification and Concentration",
      "content": "### Problem 20: Optimal Threshold and Hoeffding for Cost\n\n**Description:**\nYou have a model to detect defective products.\n\n* Cost for False Positive (FP): 5 (unnecessary inspection)\n* Cost for False Negative (FN): 50 (defective product reaches customer)\n* TP/TN cost: 0\nYou have validation data with probabilities `p` and true labels `y`. \n\n**Tasks:**\n\n1. Find the threshold $t$ that minimizes the average cost per product. \n\n2. Calculate a 99% confidence interval for the expected cost using Hoeffding's inequality. \n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Simulate data\nnp.random.seed(42)\nn_val = 2000\ny_val = np.random.binomial(1, 0.1, n_val)\np_val = np.random.uniform(0, 1, n_val)\np_val[y_val == 1] = np.random.beta(5, 2, np.sum(y_val == 1))\np_val[y_val == 0] = np.random.beta(2, 5, np.sum(y_val == 0))\n\ndef get_cost_vector(y_true, p_pred, threshold):\n    y_pred = (p_pred >= threshold).astype(int)\n    # Cost per observation\n    costs = np.zeros(len(y_true))\n    costs[(y_pred == 1) & (y_true == 0)] = 5  # FP\n    costs[(y_pred == 0) & (y_true == 1)] = 50 # FN\n    return costs\n\n# 1. Optimize threshold\nthresholds = np.linspace(0, 1, 101)\navg_costs = [np.mean(get_cost_vector(y_val, p_val, t)) for t in thresholds]\nbest_t = thresholds[np.argmin(avg_costs)]\nmin_avg_cost = np.min(avg_costs)\n\nprint(f\"Optimal Threshold: {best_t}\")\nprint(f\"Min Average Cost: {min_avg_cost:.4f}\")\n\n# 2. Hoeffding CI for 99% confidence\n# Cost C is bounded between [0, 50]. \n# Hoeffding: P(|mean(C) - E[C]| >= epsilon) <= 2 * exp(-2 * n * epsilon^2 / (b-a)^2)\nalpha = 0.01\nn = n_val\na, b = 0, 50\nepsilon = np.sqrt(((b - a)**2 * np.log(2 / alpha)) / (2 * n))\n\nci = (max(0, min_avg_cost - epsilon), min_avg_cost + epsilon)\nprint(f\"99% Confidence Interval for expected cost: {ci}\")\n\n```\n\n---\n\n## Category: Probability (Bayes' Theorem)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p21",
      "title": "Problem 21: Conditional Probability for \"Expert Knowledge\"",
      "category": "Probability (Bayes' Theorem)",
      "content": "### Problem 21: Conditional Probability for \"Expert Knowledge\"\n\n**Description:**\nA student takes an exam with 15 questions (Yes/No). \nThe number of questions the student *actually knows* is $N$.\nFor questions they don't know, they guess (50% correct).\nLet $Y$ be the total number correct. \n\n**Tasks:**\n\n1. If the student got 12 correct ($Y=12$), what is the probability that they actually *knew* fewer than 10 questions ($N < 10$)? \n\n**Solution and Code:**\n\n```python\nfrom scipy.special import binom\nimport numpy as np\n\n# P(N=k)\ndef p_N(k):\n    return binom(15, k) * (0.7**k) * (0.3**(15-k))\n\n# P(Y=12 | N=k)\n# If you know k questions, you must guess correctly on (12-k) of the remaining (15-k)\ndef p_Y_given_N(y, k):\n    if k > y: return 0\n    needed_guesses = y - k\n    remaining_q = 15 - k\n    if needed_guesses > remaining_q: return 0\n    return binom(remaining_q, needed_guesses) * (0.5**remaining_q)\n\n# P(Y=12) = sum_k P(Y=12 | N=k) * P(N=k)\np_Y_12 = sum(p_Y_given_N(12, k) * p_N(k) for k in range(16))\n\n# P(N < 10 | Y=12) = sum_{k < 10} P(Y=12 | N=k) * P(N=k) / P(Y=12)\np_N_less_10_and_Y_12 = sum(p_Y_given_N(12, k) * p_N(k) for k in range(10))\n\nresult = p_N_less_10_and_Y_12 / p_Y_12\nprint(f\"P(N < 10 | Y = 12) = {result:.4f}\")\n\n```\n\n---\n\n## Category: Covariance and Data Analysis\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p22",
      "title": "Problem 22: Geometric Interpretation of Covariance",
      "category": "Covariance and Data Analysis",
      "content": "### Problem 22: Geometric Interpretation of Covariance\n\n**Description:**\nYou have two variables $X$ and $Y$. You are told that their covariance matrix is:\n\n$$\n\\begin{pmatrix}\n4 & 1.5 \\\\\n1.5 & 1\n\\end{pmatrix}\n$$\n\n**Tasks:**\n\n1. What is the correlation $\\rho_{XY}$? \n\n2. If we transform the data to $Z = 2X - 3Y$, what is the variance of $Z$? \n\n**Answer:**\n\n1. $\\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}$.\n2. $\\text{Var}(Z) = 4\\text{Var}(X) + 9\\text{Var}(Y) + 2\\cdot2\\cdot(-3)\\text{Cov}(X,Y)$.\n\n```python\nimport numpy as np\ncov_matrix = np.array([[4, 1.5], [1.5, 1]])\nvar_x = cov_matrix[0,0]\nvar_y = cov_matrix[1,1]\ncov_xy = cov_matrix[0,1]\n\ncorr = cov_xy / (np.sqrt(var_x) * np.sqrt(var_y))\nvar_z = (2**2)*var_x + ((-3)**2)*var_y + 2*2*(-3)*cov_xy\n\nprint(f\"Correlation: {corr}\")\nprint(f\"Variance of Z: {var_z}\")\n\n```\n\n# 1MS041 Exam Preparation – Massive Problem Set (English)\n\n> **Note:** This document contains a comprehensive set of practice problems and solutions designed to mirror the structure and complexity of 1MS041 exams and assignments.  \n> Citations: [1], [3], [4], [6], [7], [8], [9]\n\n---\n\n## Category: Markov Chains\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p23",
      "title": "Problem 23: Cloud Infrastructure States",
      "category": "Markov Chains",
      "content": "### Problem 23: Cloud Infrastructure States\n\n**Description:**  \nA cloud server can be in three states: **Active (0)**, **Maintenance (1)**, and **Rebooting (2)**.  \nTransition probabilities:\n- $P(0 \\to 0) = 0.9$, $P(0 \\to 1) = 0.08$, $P(0 \\to 2) = 0.02$\n- $P(1 \\to 0) = 0.7$, $P(1 \\to 1) = 0.2$, $P(1 \\to 2) = 0.1$\n- $P(2 \\to 0) = 1.0$, $P(2 \\to 1) = 0$, $P(2 \\to 2) = 0$\n\n**Tasks:**\n1. Compute the stationary distribution $\\pi$.\n2. If the server is in Maintenance, what is the probability it is Active after 2 hours?\n3. Calculate the expected hitting time to state 2 starting from state 0.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nP = np.array([\n    [0.9, 0.08, 0.02],\n    [0.7, 0.2, 0.1],\n    [1.0, 0, 0]\n])\n\n# Stationary Distribution\nA = P.T - np.eye(3)\nA[-1] = np.ones(3)\nb = np.array([0, 0, 1])\npi = np.linalg.solve(A, b)\nprint(f\"Stationary Distribution: {pi}\")\n\n# Probability (1 -> 0) after 2 steps\nP2 = np.linalg.matrix_power(P, 2)\nprint(f\"P(X_2 = 0 | X_0 = 1) = {P2[1, 0]:.4f}\")\n\n# Expected Hitting Time to Rebooting (2) from Active (0)\nQ = P[:2, :2]\nI = np.eye(2)\nhitting_times = np.linalg.solve(I - Q, np.ones(2))\nprint(f\"Expected steps to state 2 from state 0: {hitting_times[0]:.2f}\")\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p24",
      "title": "Problem 24: MLE for a Custom Density",
      "category": "Maximum Likelihood Estimation (MLE)",
      "content": "### Problem 24: MLE for a Custom Density\n\n**Description:**  \nIID samples from PDF:  \n$f(x; \\alpha) = \\alpha^2 x e^{-\\alpha x}, \\quad x > 0, \\alpha > 0$\n\n**Tasks:**\n1. Derive the log-likelihood function $\\ell(\\alpha)$.\n2. Find the analytical MLE $\\hat{\\alpha}$.\n3. Numerically estimate $\\hat{\\alpha}$ for $x = [0.5, 1.0, 1.5, 2.0]$.\n\n**Solution:**\n- $\\ell(\\alpha) = 2n \\log(\\alpha) + \\sum \\log(x_i) - \\alpha \\sum x_i$\n- $\\hat{\\alpha} = 2 / \\bar{x}$\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndata = np.array([0.5, 1.0, 1.5, 2.0])\nalpha_hat_analytical = 2 / np.mean(data)\nprint(f\"Analytical MLE: {alpha_hat_analytical:.4f}\")\n\ndef neg_log_l(alpha, x):\n    if alpha <= 0: return 1e10\n    return -(2 * len(x) * np.log(alpha) + np.sum(np.log(x)) - alpha * np.sum(x))\n\nres = minimize(neg_log_l, x0=[1.0], args=(data,))\nprint(f\"Numerical MLE: {res.x[0]:.4f}\")\n```\n\n---\n\n## Category: Rejection Sampling\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p25",
      "title": "Problem 25: Sampling from a Triangle Distribution",
      "category": "Rejection Sampling",
      "content": "### Problem 25: Sampling from a Triangle Distribution\n\n**Description:**  \nSample 100,000 from $f(x) = 2x$ for $x \\in [0,1]$ using Uniform(0,1) proposal.\n\n**Tasks:**\n1. Determine the constant $M$.\n2. Implement rejection sampling.\n3. Approximate $E[e^X]$ using the samples.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nM = 2\ndef f(x): return 2 * x\n\ndef rejection_sampling(n):\n    samples = []\n    while len(samples) < n:\n        x_prop = np.random.uniform(0, 1)\n        u = np.random.uniform(0, 1)\n        if u <= f(x_prop) / M:\n            samples.append(x_prop)\n    return np.array(samples)\n\nsamples = rejection_sampling(100000)\nintegral_approx = np.mean(np.exp(samples))\nprint(f\"Approximate Integral: {integral_approx:.4f}\")\n```\n\n---\n\n## Category: Concentration of Measure\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p26",
      "title": "Problem 26: Hoeffding Bound for Mean Absolute Error",
      "category": "Concentration of Measure",
      "content": "### Problem 26: Hoeffding Bound for Mean Absolute Error\n\n**Description:**  \nRegression model on $n$ points, absolute error $E_i \\in [0,10]$, observed mean error (MAE) is 1.5.\n\n**Tasks:**\n1. Construct a 95% confidence interval for the true expected MAE using Hoeffding's inequality.\n2. How many samples $n$ are needed to ensure the interval width is less than 0.5?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nn = 500\na, b = 0, 10\nalpha = 0.05\nepsilon = np.sqrt(((b - a)**2 * np.log(2 / alpha)) / (2 * n))\nmae_emp = 1.5\nci = (max(0, mae_emp - epsilon), mae_emp + epsilon)\nprint(f\"95% CI for MAE: {ci}\")\n\nn_needed = ((b-a)**2 * np.log(2/alpha)) / (2 * 0.25**2)\nprint(f\"Samples needed: {int(np.ceil(n_needed))}\")\n```\n\n---\n\n## Category: Classification Performance\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p27",
      "title": "Problem 27: Precision-Recall under Class Imbalance",
      "category": "Classification Performance",
      "content": "### Problem 27: Precision-Recall under Class Imbalance\n\n**Description:**  \nDataset: 90% \"Negative\", 10% \"Positive\".  \nConfusion matrix:\n- TP = 80, FN = 20\n- FP = 100, TN = 800\n\n**Tasks:**\n1. Calculate Precision and Recall for the Positive class.\n2. Calculate F1-score.\n3. If the cost of a False Negative is 10x the cost of a False Positive, should we decrease the threshold?\n\n**Solution:**\n- Precision = $80 / (80 + 100) \\approx 0.444$\n- Recall = $80 / (80 + 20) = 0.8$\n- F1 = $2 \\cdot (0.444 \\cdot 0.8) / (0.444 + 0.8) \\approx 0.571$\n- Yes, decrease the threshold to reduce costly FNs.\n\n---\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p28",
      "title": "Problem 28: Expected Steps in a Random Walk",
      "category": "Classification Performance",
      "content": "### Problem 28: Expected Steps in a Random Walk\n\n**Description:**  \nParticle on states $\\{0,1,2,3\\}$.  \nFrom 1: to 0 (0.5), stays (0.2), to 2 (0.3).  \nFrom 2: to 1 (0.5), stays (0.2), to 3 (0.3).  \nStates 0 and 3 are absorbing.\n\n**Task:**  \nCalculate expected steps to reach 3 from state 1.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nP = np.array([\n    [1, 0, 0, 0],\n    [0.5, 0.2, 0.3, 0],\n    [0, 0.5, 0.2, 0.3],\n    [0, 0, 0, 1]\n])\n\nQ = P[1:3, 1:3]\nI = np.eye(2)\nN = np.linalg.inv(I - Q)\nexpected_steps = N.dot(np.ones(2))\nprint(f\"Expected steps to absorption from state 1: {expected_steps[0]:.2f}\")\nprint(f\"Expected steps to absorption from state 2: {expected_steps[1]:.2f}\")\n```\n\n---\n\n## Category: Concentration & VC Dimension\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p29",
      "title": "Problem 29: VC-Dimension Bounds",
      "category": "Concentration & VC Dimension",
      "content": "### Problem 29: VC-Dimension Bounds\n\n**Description:**  \nHypothesis class $H$ has VC-dimension $d$.  \n$n$ training samples, training error $err_{train}$.\n\n**Tasks:**\n1. Bound for true error: $err_{train} + \\sqrt{ (d (\\log(2n/d) + 1) + \\log(4/\\alpha)) / n }$\n2. Test set of size $n_{test}$, error $err_{test}$, Hoeffding bound for true error?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nd = 3\nn = 1000\nerr_train = 0.02\nalpha = 0.05\npenalty = np.sqrt( (d * (np.log(2*n/d) + 1) + np.log(4/alpha)) / n )\nvc_bound = err_train + penalty\nprint(f\"VC True Error Bound: {vc_bound:.4f}\")\n\nn_test = 200\nerr_test = 0.03\nepsilon_hoeff = np.sqrt( np.log(2/alpha) / (2 * n_test) )\nhoeff_bound = err_test + epsilon_hoeff\nprint(f\"Test Set Hoeffding Bound: {hoeff_bound:.4f}\")\n```\n\n---\n\n## Category: Data Transformations\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p30",
      "title": "Problem 30: Wind Velocity Covariance",
      "category": "Data Transformations",
      "content": "### Problem 30: Wind Velocity Covariance\n\n**Description:**  \nWind direction $\\theta$ (degrees), speed $v$.  \nData: $[(90, 5), (180, 10), (270, 5)]$\n\n**Tasks:**\n1. Convert to Cartesian coordinates $(v_x, v_y)$ (use radians).\n2. Compute empirical covariance matrix of velocity vectors.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\ndata = [(90, 5), (180, 10), (270, 5)]\nvectors = []\nfor deg, v in data:\n    rad = np.radians(deg)\n    vectors.append([v * np.cos(rad), v * np.sin(rad)])\n\nV = np.array(vectors)\nprint(f\"Velocity vectors:\\n{V}\")\n\ncov_matrix = np.cov(V, rowvar=False, bias=True)\nprint(f\"Empirical Covariance Matrix:\\n{cov_matrix}\")\n```\n\n# 1MS041 Master Practice Set - Core Exam Patterns (English)\n\n[cite_start]This collection focuses on the specific \"Core Problems\" that appear repeatedly in the 1MS041 exams: Markov Chains (transitions and hitting times), Binomial Probability (student/exam logic), MLE (analytical and numerical), and Concentration Bounds (Hoeffding)[cite: 1, 7, 10, 11].\n\n---\n\n## Category: Markov Chains (Hitting Times & Transitions)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p31",
      "title": "Problem 31: Website Navigation Analysis (Exam Pattern)",
      "category": "Markov Chains (Hitting Times & Transitions)",
      "content": "### Problem 31: Website Navigation Analysis (Exam Pattern)\n**Description:**\nA user on a news site moves between: **Home (0)**, **Article (1)**, and **Subscription Page (2)**.\nThe transition matrix is estimated as:\n$$\nP = \\begin{pmatrix}\n0.4 & 0.5 & 0.1 \\\\\n0.3 & 0.6 & 0.1 \\\\\n0.0 & 0.0 & 1.0\n\\end{pmatrix}\n$$\n[cite_start]Note: The \"Subscription Page\" (2) is an absorbing state[cite: 10].\n\n**Tasks:**\n1. [cite_start]Calculate the expected number of steps until a user reaches the Subscription Page starting from Home[cite: 7].\n2. [cite_start]If a user starts at Home, what is the probability they are reading an Article after 2 steps[cite: 10]?\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# Transition Matrix\nP = np.array([\n    [0.4, 0.5, 0.1],\n    [0.3, 0.6, 0.1],\n    [0.0, 0.0, 1.0]\n])\n\n# 1. Hitting Time to State 2 (Absorbing)\nQ = P[0:2, 0:2]\nI = np.eye(2)\nN = np.linalg.inv(I - Q)\nexpected_steps = N.dot(np.ones(2))\nprint(f\"Expected steps from Home (0) to Subscription (2): {expected_steps[0]:.2f}\")\n\n# 2. Probability Home -> Article after 2 steps\nP2 = np.linalg.matrix_power(P, 2)\nprint(f\"P(X_2 = 1 | X_0 = 0) = {P2[0, 1]:.4f}\")\n```\n\n---\n\n## Category: Probability & Bayes (The \"Exam/Student\" Pattern)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p32",
      "title": "Problem 32: Quality Inspection (Pattern: Assignment 1, Problem 4)",
      "category": "Probability & Bayes (The \"Exam/Student\" Pattern)",
      "content": "### Problem 32: Quality Inspection (Pattern: Assignment 1, Problem 4)\n\n**Description:**\nA factory produces batches. The number of defective items $N$. An inspector checks the batch. If they find $\\geq 2$ defects, the batch is rejected. However, the inspector only detects a defect with 80% probability. For healthy items, there is a 5% \"false alarm\" rate where the inspector thinks it's defective.\n\n**Tasks:**\n1. Compute the probability that a batch actually has $<2$ defects given that it was rejected ($Y \\geq 2$).\n\n**Solution and Code:**\n\n```python\nfrom scipy.special import binom\nimport numpy as np\n\nn_total = 10\np_N = lambda k: binom(n_total, k) * (0.2**k) * (0.8**(n_total-k))\n\n# P(Y >= 2 | N = k)\ndef p_rejected_given_N(k):\n    n_sim = 20000\n    tp = np.random.binomial(k, 0.8, n_sim)\n    fp = np.random.binomial(n_total - k, 0.05, n_sim)\n    y = tp + fp\n    return np.mean(y >= 2)\n\np_Y_ge_2 = sum(p_rejected_given_N(k) * p_N(k) for k in range(n_total + 1))\np_N_less_2_and_Y_ge_2 = sum(p_rejected_given_N(k) * p_N(k) for k in range(2))\nresult = p_N_less_2_and_Y_ge_2 / p_Y_ge_2\nprint(f\"P(N < 2 | Rejected) = {result:.4f}\")\n```\n\n---\n\n## Category: MLE (Analytical and Numerical)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p33",
      "title": "Problem 33: MLE for Poisson (Pattern: Exam June 2023, Problem 3)",
      "category": "MLE (Analytical and Numerical)",
      "content": "### Problem 33: MLE for Poisson (Pattern: Exam June 2023, Problem 3)\n\n**Description:**\nA healthcare organization models physician visits using a Poisson distribution where $Y_i \\sim \\text{Poisson}(\\lambda_i)$, $\\lambda_i = \\exp(X_i \\beta)$.\n\n**Tasks:**\n1. Derive the negative log-likelihood for $n$ observations.\n2. Implement the `loss` function for optimization.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nclass PoissonRegression:\n    def loss(self, coeffs, X, Y):\n        lam = np.exp(np.dot(X, coeffs))\n        log_l = np.sum(Y * np.dot(X, coeffs) - lam)\n        return -log_l\n\n# Test\nX = np.array([[1, 2], [1, 3], [1, 1]])\nY = np.array([5, 10, 2])\nmodel = PoissonRegression()\nprint(f\"Loss for [0.5, 0.2]: {model.loss(np.array([0.5, 0.2]), X, Y):.4f}\")\n```\n\n---\n\n## Category: Sampling & Monte Carlo Integration\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p34",
      "title": "Problem 34: Semicircle Distribution (Pattern: Exam Jan 2024, Problem 1)",
      "category": "Sampling & Monte Carlo Integration",
      "content": "### Problem 34: Semicircle Distribution (Pattern: Exam Jan 2024, Problem 1)\n\n**Description:**\nGenerate 100,000 samples from the PDF $f(x) = \\frac{2}{\\pi} \\sqrt{1-x^2}$ for $x \\in [-1,1]$.\n\n**Tasks:**\n1. Use the samples to approximate $E[|X|]$.\n2. Provide a 95% confidence interval using Hoeffding's inequality.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\ndef sample_semicircle(n):\n    samples = []\n    while len(samples) < n:\n        x_prop = np.random.uniform(-1, 1)\n        u = np.random.uniform(0, 1)\n        f_val = (2/np.pi) * np.sqrt(1 - x_prop**2)\n        if u <= f_val / (2/np.pi):\n            samples.append(x_prop)\n    return np.array(samples)\n\nsamples = sample_semicircle(100000)\nh_samples = np.abs(samples)\nintegral_est = np.mean(h_samples)\n\nn = 100000\nepsilon = np.sqrt(np.log(2/0.05) / (2 * n))\nprint(f\"Integral Estimate: {integral_est:.4f}\")\nprint(f\"95% CI: [{integral_est - epsilon:.4f}, {integral_est + epsilon:.4f}]\")\n```\n\n---\n\n## General Strategy for 1MS041 Exams\n\nTo succeed in this course, follow these general approaches for recurring problem types:\n\n### 1. Markov Chain Problems\n\n* **Stationary Distribution**: Always check if $\\pi P = \\pi$. In Python, use `np.linalg.solve(P.T - np.eye(n).T, b)` where the last row of the system is replaced by the sum condition $\\sum \\pi_i = 1$.\n* **Hitting Times**: Identify absorbing vs. transient states. Use the fundamental matrix $N = (I - Q)^{-1}$ where $Q$ contains only transitions between non-absorbing states.\n\n### 2. Maximum Likelihood (MLE)\n\n* **Analytical**: Write the likelihood $L(\\theta)$, take $\\log L$, differentiate, and set to zero. Common distributions: Normal, Exponential, Poisson, and Rayleigh.\n* **Numerical**: Use `scipy.optimize.minimize`. **Critical**: Always add a small `epsilon` or bounds to prevent `log(0)` or `sqrt(negative)` errors.\n\n### 3. Sampling & Integration\n\n* **Inversion**: If the CDF $F(x)$ is easy to invert, use $F^{-1}(u)$.\n* **Rejection**: Find $M$ such that $f(x) \\leq M g(x)$. Usua",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p35",
      "title": "Problem 35: The \"Glider\" Communication Model",
      "category": "Markov Chains & Expected Steps",
      "content": "### Problem 35: The \"Glider\" Communication Model\n**Description:**\nA communication packet is transmitted. It can be in three states: **In Transit (0)**, **Corrupted (1)**, or **Delivered (2)**.\n- From **In Transit**: 70% stay in transit, 20% get corrupted, 10% are delivered.\n- From **Corrupted**: 50% are retransmitted (go to In Transit), 50% stay corrupted.\n- From **Delivered**: This is an absorbing state ($P_{22} = 1$).\n\n**Tasks:**\n1. [cite_start]Construct the transition matrix $P$[cite: 7, 11].\n2. [cite_start]Calculate the expected number of steps until a packet is Delivered, starting from \"In Transit\"[cite: 11].\n3. Find the probability the packet is delivered within 3 steps.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Transition Matrix\nP = np.array([\n    [0.7, 0.2, 0.1],\n    [0.5, 0.5, 0.0],\n    [0.0, 0.0, 1.0]\n])\n\n# 2. Expected Hitting Time to Delivered (State 2)\nQ = P[0:2, 0:2]\nI = np.eye(2)\nN = np.linalg.inv(I - Q)\nexpected_steps = N.dot(np.ones(2))\n\nprint(f\"Expected steps to Delivery from Transit: {expected_steps[0]:.2f}\")\nprint(f\"Expected steps to Delivery from Corrupted: {expected_steps[1]:.2f}\")\n\n# 3. Probability Delivered within 3 steps\nP3 = np.linalg.matrix_power(P, 3)\nprint(f\"P(Delivered by step 3) = {P3[0, 2]:.4f}\")\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p36",
      "title": "Problem 36: MLE for a Truncated Exponential (Pattern: Exam 2023)",
      "category": "Maximum Likelihood Estimation (MLE)",
      "content": "### Problem 36: MLE for a Truncated Exponential (Pattern: Exam 2023)\n\n**Description:**\nObservations $x_i$ follow $f(x; \\lambda) = \\lambda e^{-\\lambda x} / (1 - e^{-\\lambda})$ for $x \\in [0, 1]$.\n\n**Tasks:**\n1. Implement the negative log-likelihood function.\n2. Solve for $\\lambda$ numerically using the data $[0.2, 0.5, 0.1, 0.4, 0.3]$.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\n\ndata = np.array([0.2, 0.5, 0.1, 0.4, 0.3])\n\ndef neg_log_likelihood(lam, x):\n    if lam <= 0: return 1e10\n    n = len(x)\n    log_l = n*np.log(lam) - lam*np.sum(x) - n*np.log(1 - np.exp(-lam))\n    return -log_l\n\nres = optimize.minimize_scalar(neg_log_likelihood, args=(data,), bounds=(0.01, 20), method='bounded')\nprint(f\"Numerical MLE lambda_hat: {res.x:.4f}\")\n```\n\n---\n\n## Category: Sampling & Monte Carlo Integration\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p37",
      "title": "Problem 37: Complex Inversion Sampling (Pattern: Exam Jan 2024)",
      "category": "Sampling & Monte Carlo Integration",
      "content": "### Problem 37: Complex Inversion Sampling (Pattern: Exam Jan 2024)\n\n**Description:**\nGenerate 100,000 samples for the CDF $F(x) = \\frac{e^{x^2} - 1}{e - 1}$ for $x \\in [0, 1]$.\n\n**Tasks:**\n1. Derive $F^{-1}(u)$.\n2. Estimate the integral $\\int_0^1 \\sin(x) f(x) dx$.\n3. Construct a 95% confidence interval using Hoeffding's inequality.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Inversion\n# u = (exp(x^2)-1)/(e-1) => x = sqrt(ln(u(e-1) + 1))\ndef inv_f(u):\n    return np.sqrt(np.log(u * (np.e - 1) + 1))\n\nn = 100000\nu_samples = np.random.uniform(0, 1, n)\nx_samples = inv_f(u_samples)\n\n# 2. Monte Carlo Integration\nintegral_est = np.mean(np.sin(x_samples))\n\n# 3. Hoeffding CI\nepsilon = np.sqrt(np.log(2/0.05) / (2 * n))\nprint(f\"Estimated Integral: {integral_est:.4f}\")\nprint(f\"95% CI: [{integral_est - epsilon:.4f}, {integral_est + epsilon:.4f}]\")\n```\n\n---\n\n## Category: Concentration of Measure (Logic)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p38",
      "title": "Problem 38: Speed of Convergence (Pattern: Assignment 1)",
      "category": "Concentration of Measure (Logic)",
      "content": "### Problem 38: Speed of Convergence (Pattern: Assignment 1)\n\n**Description:**\nWhich of the following will concentrate **exponentially** (e.g., $P(|\\bar{X}_n - \\mu| > \\epsilon) \\leq 2e^{-cn\\epsilon^2}$)?\n\n1. Empirical mean of i.i.d. Sub-Gaussian variables.\n2. Empirical mean of i.i.d. variables with finite variance.\n3. Empirical mean of i.i.d. Cauchy variables.\n4. Empirical mean of i.i.d. Bernoulli variables.\n\n**Answer:**\n\n* **1 and 4** concentrate exponentially.\n* 2 concentrates polynomially (Chebyshev).\n* 3 does not concentrate at all.\n\n---\n\n## Category: Classification & Risk\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p39",
      "title": "Problem 39: Optimal Threshold Calculation",
      "category": "Classification & Risk",
      "content": "### Problem 39: Optimal Threshold Calculation\n\n**Description:**\nFraud detection model:\n\n* Costs: $C_{FN}$, $C_{FP}$.\n* Target $P(Y=1)$.\n* Model output: $p$.\n\n**Task:**\nFind the theoretical threshold $t^*$.\n\n**Solution:**\nRisk for predicting 1: $C_{FP} \\cdot P(Y=0|p)$.\nRisk for predicting 0: $C_{FN} \\cdot P(Y=1|p)$.\nPredict 1 if $C_{FP} \\cdot (1-p) < C_{FN} \\cdot p$.\n$t^* = \\frac{C_{FP}}{C_{FP} + C_{FN}}$.\n\n---\n\n## Category: Confidence Intervals\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p40",
      "title": "Problem 40: Hoeffding vs. Chebyshev",
      "category": "Confidence Intervals",
      "content": "### Problem 40: Hoeffding vs. Chebyshev\n\n**Description:**\nYou observe 1000 coin flips and get 550 heads. Estimate $p$ and give a 95% CI.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nn = 1000\np_hat = 0.55\n\n# Hoeffding CI (Bounded [0, 1])\neps_h = np.sqrt(np.log(2/0.05) / (2 * n))\nprint(f\"Hoeffding CI: [{p_hat - eps_h:.4f}, {p_hat + eps_h:.4f}]\")\n\n# Chebyshev CI (Variance p(1-p) <= 0.25)\neps_c = np.sqrt(0.25 / (n * 0.05))\nprint(f\"Chebyshev CI: [{p_hat - eps_c:.4f}, {p_hat + eps_c:.4f}]\")\n```\n\n**Next Step**: Would you like me to create a focused# 1MS041 Advanced Practice Set - Pattern Recognition & Implementation (English)\n\n[cite_start]This set focuses on high-yield exam patterns derived from previous assessments[cite: 1, 10, 11].\n\n---\n\n## Category: Markov Chains & Expected Steps\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p35",
      "title": "Problem 35: The \"Glider\" Communication Model",
      "category": "Markov Chains & Expected Steps",
      "content": "### Problem 35: The \"Glider\" Communication Model\n**Description:**\nA communication packet is transmitted. It can be in three states: **In Transit (0)**, **Corrupted (1)**, or **Delivered (2)**.\n- From **In Transit**: 70% stay in transit, 20% get corrupted, 10% are delivered.\n- From **Corrupted**: 50% are retransmitted (go to In Transit), 50% stay corrupted.\n- From **Delivered**: This is an absorbing state ($P_{22} = 1$).\n\n**Tasks:**\n1. [cite_start]Construct the transition matrix $P$[cite: 7, 11].\n2. [cite_start]Calculate the expected number of steps until a packet is Delivered, starting from \"In Transit\"[cite: 11].\n3. Find the probability the packet is delivered within 3 steps.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Transition Matrix\nP = np.array([\n    [0.7, 0.2, 0.1],\n    [0.5, 0.5, 0.0],\n    [0.0, 0.0, 1.0]\n])\n\n# 2. Expected Hitting Time to Delivered (State 2)\nQ = P[0:2, 0:2]\nI = np.eye(2)\nN = np.linalg.inv(I - Q)\nexpected_steps = N.dot(np.ones(2))\n\nprint(f\"Expected steps to Delivery from Transit: {expected_steps[0]:.2f}\")\nprint(f\"Expected steps to Delivery from Corrupted: {expected_steps[1]:.2f}\")\n\n# 3. Probability Delivered within 3 steps\nP3 = np.linalg.matrix_power(P, 3)\nprint(f\"P(Delivered by step 3) = {P3[0, 2]:.4f}\")\n```\n\n---\n\n## Category: Maximum Likelihood Estimation (MLE)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p36",
      "title": "Problem 36: MLE for a Truncated Exponential (Pattern: Exam 2023)",
      "category": "Maximum Likelihood Estimation (MLE)",
      "content": "### Problem 36: MLE for a Truncated Exponential (Pattern: Exam 2023)\n\n**Description:**\nObservations $x_i$ follow $f(x; \\lambda) = \\lambda e^{-\\lambda x} / (1 - e^{-\\lambda})$ for $x \\in [0, 1]$.\n\n**Tasks:**\n1. Implement the negative log-likelihood function.\n2. Solve for $\\lambda$ numerically using the data $[0.2, 0.5, 0.1, 0.4, 0.3]$.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\nfrom scipy import optimize\n\ndata = np.array([0.2, 0.5, 0.1, 0.4, 0.3])\n\ndef neg_log_likelihood(lam, x):\n    if lam <= 0: return 1e10\n    n = len(x)\n    log_l = n*np.log(lam) - lam*np.sum(x) - n*np.log(1 - np.exp(-lam))\n    return -log_l\n\nres = optimize.minimize_scalar(neg_log_likelihood, args=(data,), bounds=(0.01, 20), method='bounded')\nprint(f\"Numerical MLE lambda_hat: {res.x:.4f}\")\n```\n\n---\n\n## Category: Sampling & Monte Carlo Integration\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p37",
      "title": "Problem 37: Complex Inversion Sampling (Pattern: Exam Jan 2024)",
      "category": "Sampling & Monte Carlo Integration",
      "content": "### Problem 37: Complex Inversion Sampling (Pattern: Exam Jan 2024)\n\n**Description:**\nGenerate 100,000 samples for the CDF $F(x) = \\frac{e^{x^2} - 1}{e - 1}$ for $x \\in [0, 1]$.\n\n**Tasks:**\n1. Derive $F^{-1}(u)$.\n2. Estimate the integral $\\int_0^1 \\sin(x) f(x) dx$.\n3. Construct a 95% confidence interval using Hoeffding's inequality.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\n# 1. Inversion\n# u = (exp(x^2)-1)/(e-1) => x = sqrt(ln(u(e-1) + 1))\ndef inv_f(u):\n    return np.sqrt(np.log(u * (np.e - 1) + 1))\n\nn = 100000\nu_samples = np.random.uniform(0, 1, n)\nx_samples = inv_f(u_samples)\n\n# 2. Monte Carlo Integration\nintegral_est = np.mean(np.sin(x_samples))\n\n# 3. Hoeffding CI\nepsilon = np.sqrt(np.log(2/0.05) / (2 * n))\nprint(f\"Estimated Integral: {integral_est:.4f}\")\nprint(f\"95% CI: [{integral_est - epsilon:.4f}, {integral_est + epsilon:.4f}]\")\n```\n\n---\n\n## Category: Concentration of Measure (Logic)\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p38",
      "title": "Problem 38: Speed of Convergence (Pattern: Assignment 1)",
      "category": "Concentration of Measure (Logic)",
      "content": "### Problem 38: Speed of Convergence (Pattern: Assignment 1)\n\n**Description:**\nWhich of the following will concentrate **exponentially** (e.g., $P(|\\bar{X}_n - \\mu| > \\epsilon) \\leq 2e^{-cn\\epsilon^2}$)?\n\n1. Empirical mean of i.i.d. Sub-Gaussian variables.\n2. Empirical mean of i.i.d. variables with finite variance.\n3. Empirical mean of i.i.d. Cauchy variables.\n4. Empirical mean of i.i.d. Bernoulli variables.\n\n**Answer:**\n\n* **1 and 4** concentrate exponentially.\n* 2 concentrates polynomially (Chebyshev).\n* 3 does not concentrate at all.\n\n---\n\n## Category: Classification & Risk\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p39",
      "title": "Problem 39: Optimal Threshold Calculation",
      "category": "Classification & Risk",
      "content": "### Problem 39: Optimal Threshold Calculation\n\n**Description:**\nFraud detection model:\n\n* Costs: $C_{FN}$, $C_{FP}$.\n* Target $P(Y=1)$.\n* Model output: $p$.\n\n**Task:**\nFind the theoretical threshold $t^*$.\n\n**Solution:**\nRisk for predicting 1: $C_{FP} \\cdot P(Y=0|p)$.\nRisk for predicting 0: $C_{FN} \\cdot P(Y=1|p)$.\nPredict 1 if $C_{FP} \\cdot (1-p) < C_{FN} \\cdot p$.\n$t^* = \\frac{C_{FP}}{C_{FP} + C_{FN}}$.\n\n---\n\n## Category: Confidence Intervals\n\n",
      "type": "study-guide"
    },
    {
      "id": "study-guide-p40",
      "title": "Problem 40: Hoeffding vs. Chebyshev",
      "category": "Confidence Intervals",
      "content": "### Problem 40: Hoeffding vs. Chebyshev\n\n**Description:**\nYou observe 1000 coin flips and get 550 heads. Estimate $p$ and give a 95% CI.\n\n**Solution and Code:**\n\n```python\nimport numpy as np\n\nn = 1000\np_hat = 0.55\n\n# Hoeffding CI (Bounded [0, 1])\neps_h = np.sqrt(np.log(2/0.05) / (2 * n))\nprint(f\"Hoeffding CI: [{p_hat - eps_h:.4f}, {p_hat + eps_h:.4f}]\")\n\n# Chebyshev CI (Variance p(1-p) <= 0.25)\neps_c = np.sqrt(0.25 / (n * 0.05))\nprint(f\"Chebyshev CI: [{p_hat - eps_c:.4f}, {p_hat + eps_c:.4f}]\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# QUESTIONS AND ANSWERS (BIG)\n\nThis file contains all questions from `EXAM_QUESTION_BANK.md` followed by worked, self-contained solutions. I will progressively fill sections with complete solutions; the file starts with the full question bank and then SOLUTIONS for the first major section.\n\n---\n\n<!-- BEGIN COPIED QUESTION BANK -->\n\n# COURSE 1MS041 - COMPREHENSIVE EXAM QUESTION BANK\n## Introduction to Data Science\n\n**DISCLAIMER:** This document contains ONLY questions extracted from previous exams, assignments, and lecture materials. No solutions are provided. Use this as a comprehensive study guide by searching for topics.\n\n---\n\n## TABLE OF CONTENTS\n1. [PROBABILITY THEORY](#probability-theory)\n2. [RANDOM VARIABLES](#random-variables)\n3. [MARKOV CHAINS](#markov-chains)\n4. [CONCENTRATION OF MEASURE & LIMITS](#concentration-of-measure--limits)\n5. [STATISTICAL ESTIMATION & MAXIMUM LIKELIHOOD](#statistical-estimation--maximum-likelihood)\n6. [RANDOM NUMBER GENERATION & SAMPLING](#random-number-generation--sampling)\n7. [REGRESSION MODELS](#regression-models)\n8. [LOGISTIC REGRESSION & CLASSIFICATION](#logistic-regression--classification)\n9. [MACHINE LEARNING METRICS & EVALUATION](#machine-learning-metrics--evaluation)\n10. [CALIBRATION & THRESHOLD OPTIMIZATION](#calibration--threshold-optimization)\n11. [RISK & DECISION THEORY](#risk--decision-theory)\n12. [POISSON REGRESSION](#poisson-regression)\n13. [TEXT CLASSIFICATION & NLP](#text-",
      "type": "study-guide"
    }
  ]
}